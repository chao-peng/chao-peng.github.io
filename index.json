[{"authors":["admin"],"categories":null,"content":"Chao Peng （彭超） is a PhD candidate at the Laboratory for Foundations of Computer Science (LFCS), School of Informatics, The University of Edinburgh. He holds an MSc in High Performance Computing with Data Science from the University of Edinburgh and a BEng in Computer Science and Technology from Xuzhou University of Technology. During his MSc, he was a member of Team EPCC for the International Supercomputing Conference Student Cluster Competition.\nHis research interest lies in the area of massively parallel architectures and programming. He is currently doing research in defining code coverage metrics for GPU programs and automated test case generation, reduction and execution. His supervisor is Dr. Ajitha Rajan.\nEmail: ku.ca.de@gnep.oahc\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1561550524,"objectID":"3eef613d86e546bf0ce1d3fb271035e3","permalink":"https://chao-peng.github.io/author/chao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chao/","section":"author","summary":"Chao Peng （彭超） is a PhD candidate at the Laboratory for Foundations of Computer Science (LFCS), School of Informatics, The University of Edinburgh. He holds an MSc in High Performance Computing with Data Science from the University of Edinburgh and a BEng in Computer Science and Technology from Xuzhou University of Technology. During his MSc, he was a member of Team EPCC for the International Supercomputing Conference","tags":null,"title":"Chao Peng","type":"author"},{"authors":["Chao Peng","Sefa Akca","Ajitha Rajan"],"categories":null,"content":"","date":1569798000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561550524,"objectID":"9f921ef3705e0d431e799e2f871b590d","permalink":"https://chao-peng.github.io/publication/sif/","publishdate":"2019-09-30T00:00:00+01:00","relpermalink":"/publication/sif/","section":"publication","summary":"Solidity is an object-oriented and high-level language for writing smart contracts that are used to execute, verify and enforce credible transactions on permissionless blockchains. In the last few years, analysis of smart contracts has raised considerable interest and numerous techniques have been proposed to check the presence of vulnerabilities in them. Current techniques lack traceability in source code and have widely differing work flows. There is no single unifying framework for analysis, instrumentation, optimisation and code generation of Solidity contracts.\n\nIn this paper, we present SIF, a comprehensive framework for Solidity contract analysis, query, instrumentation, and code generation. SIF provides support for Solidity contract developers and testers to build source level techniques for analysis, understanding, diagnostics, optimisations and code generation. We show feasibility and applicability of the framework by building practical tools on top of it and running them on 1838 real smart contracts deployed on the Ethereum network.\n","tags":[],"title":"SIF: A Framework for Solidity Contract Instrumentation and Analysis","type":"publication"},{"authors":["Sefa Akca","Ajitha Rajan","Chao Peng"],"categories":null,"content":"","date":1569798000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569798000,"objectID":"f10cb3c35c72a89ffcbc0104521f9ee9","permalink":"https://chao-peng.github.io/publication/solanalyser/","publishdate":"2019-09-30T00:00:00+01:00","relpermalink":"/publication/solanalyser/","section":"publication","summary":"Executing, verifying and enforcing credible transactions on permissionless blockchains is done using smart contracts. Smart contracts define and execute crucial agreements, and attacks exploiting their vulnerabilities can lead to huge losses, like the failure of the DAO smart contract that resulted in 50 million US Dollars worth of losses. A key challenge with smart contracts is ensuring their correctness and security.\n\nTo address this challenge, we present a fully automated technique, SolAnalyser, for vulnerability detection over Solidity smart contracts that uses both static and dynamic analysis. Analysis techniques for smart contracts in the literature rely on static analysis with a high rate of false positives or lack support for vulnerabilities like out of gas, unchecked send, timestamp dependency. We instrument the original smart contract with property checks and use dynamic analysis to tackle this problem. Our tool, SolAnalyser, supports automated detection of 8 different vulnerability types that currently lack wide support in existing tools, and can easily be extended to support other types. We also implemented a fault seeding tool that injects different types of vulnerabilities in smart contracts. We use the mutated contracts for assessing the effectiveness of different analysis tools.\n\nOur experiment uses 1838 real contracts from which we generate 12866 mutated contracts by artificially seeding 8 different vulnerability types. We evaluate the effectiveness of our technique in revealing the seeded vulnerabilities and compare against five existing popular analysis tools -- Oyente, Securify, Maian, SmartCheck and Mythril. This is the first large scale evaluation of existing tools that compares their effectiveness by running them on a common set of contracts. We find that our technique outperforms all five existing tools in supporting detection of all 8 vulnerability types and in achieving higher precision and recall rate. SolAnalyser was also faster in analysing the different vulnerabilities than any of the existing tools in our experiment. Among existing tools, Securify achieved high precision in detecting integer overflow, underflow, and division by zero but had poor recall rates.\n","tags":[],"title":"SolAnalyser: A Framework for Analysing and Testing Smart Contracts","type":"publication"},{"authors":["Chao Peng"],"categories":null,"content":"","date":1559170800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562235842,"objectID":"bb05b16b729dbb9d601d7df746c67617","permalink":"https://chao-peng.github.io/publication/isstads/","publishdate":"2019-05-30T00:00:00+01:00","relpermalink":"/publication/isstads/","section":"publication","summary":"Testing is an important and challenging part of software development and its effectiveness depends on the quality of test cases. However, there exists no means of measuring quality of tests developed for GPU programs and as a result, no test case generation techniques for GPU programs aiming at high test effectiveness. Existing criteria for sequential and threaded CPU programs cannot be directly applied to GPU programs as GPU follows a completely different memory and\nexecution model. \n\nWe surveyed existing work on GPU program verification and bug fixes of open source GPU programs. Based on our findings, we define barrier, branch and loop coverage criteria and propose a set of mutation operators to measure fault finding capabilities of test cases. CLTestCheck, a framework for measuring quality of tests developed for GPU programs by code coverage analysis, fault seeding and work-group schedule amplification has been developed and evaluated using industry\nstandard benchmarks. Experiments show that the framework is able to automatically measure test effectiveness and reveal unusual behaviours. Our planned work includes data flow coverage adopted for GPU programs to probe the underlying cause of unusual kernel behaviours and a more comprehensive work-group scheduler. We also plan to design and develop an automatic test case generator aiming at generating high quality test suites for GPU programs.\n","tags":[],"title":"On the Correctness of GPU Programs","type":"publication"},{"authors":["Chao Peng","Ajitha Rajan"],"categories":null,"content":"","date":1554505200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561550524,"objectID":"7e0be6aad3b6512a7eefe822ae06aaae","permalink":"https://chao-peng.github.io/publication/cltestcheck/","publishdate":"2019-04-06T00:00:00+01:00","relpermalink":"/publication/cltestcheck/","section":"publication","summary":"Massive parallelism, and energy efficiency of GPUs, along with \nadvances in their programmability with OpenCL and CUDA programming models have made them attractive for general-purpose computations across many application domains.\nTechniques for testing GPU kernels have emerged recently to aid the construction of correct GPU software. \nHowever, there exists no means of measuring quality and effectiveness of tests developed for GPU kernels. \nTraditional coverage criteria over CPU programs is not adequate over GPU kernels as it uses a completely different programming model and the faults encountered may be  specific to the GPU architecture. \nGPUs have SIMT (single instruction, multiple thread) execution model that executes batches of threads (work groups) in lock-step, i.e all threads in a work group execute the same instruction but on different data.  \n\nWe address this need in this paper and present a framework, CLTestCheck, for assessing quality of test suites developed for OpenCL kernels. The framework has the following capabilities, \n1. Measures kernel code coverage using three different coverage metrics that are inspired by faults found in real kernel code, 2. Seeds different types of faults in kernel code and measures fault finding capability of test suite, 3. Simulates different work group schedules to check for potential data races with the given test suite. We conducted empirical evaluation of CLTestCheck on a collection of 82 publicly available GPU kernels and test suites. We found that CLTestCheck is capable of automatically measuring effectiveness of test suites, in terms of kernel code coverage, fault finding and revealing data races in real OpenCL kernels. \n","tags":[],"title":"CLTestCheck: Measuring Test Effectiveness for GPU Kernels","type":"publication"}]