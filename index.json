[{"authors":["admin"],"categories":null,"content":"Chao Peng （彭péng, 超chāo）\nI am a Senior Researcher at ByteDance (字节跳动) and a part-time postgraduate student mentor of School of Computer Science, Fudan University. I received my PhD degree from Laboratory for Foundations of Computer Science (LFCS), The University of Edinburgh under supervision of Dr. Ajitha Rajan.\nAt ByteDance, I am on the Software Engineering Lab, Dev Infra Research Center. My research interest lies in the area of software testing, program repair and optimisations, and the synergy with machine learning and compiler techniques. I am also responsible for academic development and university collaboration.\nI am passionate about building practical software testing, analysis, and debugging systems to predict, detect, diagnose, and fix bugs for all kinds of software systems.\nOutside of work, I enjoy going to the gym.\nEmail:\nprefix=\u0026quot;chao.peng\u0026quot; domain=\u0026quot;acm\u0026quot; email=${prefix}@${domain}.org ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1694896074,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/chao-peng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chao-peng/","section":"authors","summary":"Chao Peng （彭péng, 超chāo） I am a Senior Researcher at ByteDance (字节跳动) and a","tags":null,"title":"Chao Peng","type":"authors"},{"authors":["Chao Peng","Zhengwei Lv","Jiarong Fu","Jiayuan Liang","Zhao Zhang","Ajitha Rajan","Ping Yang"],"categories":[],"content":"","date":1712876400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705488832,"objectID":"e40b64c9b4284a02db7945323ae04ead","permalink":"/publication/hawkeye/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/hawkeye/","section":"publication","summary":"Android Apps are frequently updated to keep up with changing user, hardware, and business demands. Ensuring the correctness of App updates through extensive testing is crucial to avoid potential bugs reaching the end user. Existing Android testing tools generate GUI events that focus on improving the test coverage of the entire App rather than prioritising updates and its impacted elements. Recent research has proposed change-focused testing but relies on random exploration to exercise the updates and impacted GUI elements that is ineffective and slow for large complex Apps with a huge input exploration space. At ByteDance, our established model-based GUI testing tool, Fastbot2, has been in successful deployment for nearly three years. Fastbot2 leverages event-activity transition models derived from past App explorations to achieve enhanced test coverage efficiently. A pivotal insight we gained is that the knowledge of event-activity transitions is equally valuable in effectively targeting changes introduced by App updates. This insight propelled our proposal for directed testing of App updates with Hawkeye. Hawkeye excels in prioritizing GUI actions associated with code changes through deep reinforcement learning from historical exploration data.\nIn our empirical evaluation, we rigorously compared Hawkeye with state-of-the-art tools like Fastbot2 and ARES. We utilized 10 popular open-source Apps and a notable commercial App for this evaluation. The results showcased that Hawkeye consistently outperforms Fastbot2 and ARES in generating GUI event sequences that effectively target changed functions, both in open-source and commercial App contexts.\nIn real-world industrial deployment, Hawkeye is seamlessly integrated into our development pipeline, performing smoke testing for merge requests in a complex commercial App. The positive feedback received from our App development teams further affirmed Hawkeye's ability in testing App updates effectively.","tags":["Software Testing"],"title":"Hawkeye: Change-targeted Testing for Android Apps based on Deep Reinforcement Learning","type":"publication"},{"authors":["Xiaoyun Liang","Jiayi Qi","Yongqiang Gao","Chao Peng","Ping Yang"],"categories":[],"content":"","date":1701558000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694895869,"objectID":"55621b7637405368de293c6261514691","permalink":"/publication/ag3/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/ag3/","section":"publication","summary":"With the advancement of device software and hardware performance, and the evolution of game engines, an increasing number of emerging high-quality games are captivating game players from all around the world who speak different languages. However, due to the vast fragmentation of the device and platform market, a well-tested game may still experience text glitches when installed on a new device with an unseen screen resolution and system version, which can significantly impact the user experience. In our testing pipeline, current testing techniques for identifying multilingual text glitches are laborious and inefficient. In this paper, we present AG3, which offers intelligent game traversal, precise visual text glitch detection, and integrated quality report generation capabilities. Our empirical evaluation and internal industrial deployment demonstrate that AG3 can detect various real-world multilingual text glitches with minimal human involvement.","tags":["Software Testing"],"title":"AG3: Automated Game GUI Text Glitch Detection based on Computer Vision","type":"publication"},{"authors":["Zongze Jiang","Ming Wen","Yixin Yang","Chao Peng","Ping Yang","Hai Jin"],"categories":[],"content":"","date":1694386800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694895869,"objectID":"a1aa7af23898628abfa5cfe07063a9c6","permalink":"/publication/gopie/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/gopie/","section":"publication","summary":"The Go language (Go/Golang) has been attracting increasing attention from the industry in recent years due to its straightforward syntax, strong concurrency support, and ease of deployment. This programming language encourages developers to use channel-based concurrency, which simplifies development. Unfortunately, it also introduces new concurrency problems that differ from those caused by the mechanism of shared memory concurrency. Despite this, there are only few works that aim to detect these Go-specific concurrency issues. Even state-of-the-art testing tools will miss critical concurrent bugs that require fine-grained and effective interleaving exploration.\n\nThis paper presents GoPie, a novel testing approach for detecting Go concurrent bugs through primitive-constrained interleaving exploration. GoPie utilizes execution histories to identify new interleavings instead of relying on exhaustive exploration or random scheduling. To evaluate its effectiveness, we applied GoPie on existing benchmarks and large-scale open-source projects. Results show that GoPie can effectively explore concurrent interleavings and detect significantly more bugs in the benchmark. Furthermore, it uncovered 11 unique previously unknown concurrent bugs, and 9 of which have been confirmed.","tags":["Software Testing"],"title":"Effective Concurrency Testing for Go via Directional Primitive-constrained Interleaving Exploration","type":"publication"},{"authors":["Siwei Wang","Xue Mao","Ziguang Gao","Yujun Gao","Qucheng Shen","Chao Peng"],"categories":[],"content":"","date":1686697200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682920984,"objectID":"c4d6afb9e3ad06e0055297669d8fb9e1","permalink":"/publication/nxtunit/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/nxtunit/","section":"publication","summary":"Automated test generation has been extensively studied for dynamically compiled or typed programming languages like Java and Python. However, Go, a popular statically compiled and typed programming language for server application development, has received limited support from existing tools. To address this gap, we present NxtUnit, an automatic unit test generation tool for Go that uses random testing and is well-suited for microservice architecture. NxtUnit employs a random approach to generate unit tests quickly, making it ideal for smoke testing and providing quick quality feedback. It comes with three types of interfaces: an integrated development environment (IDE) plugin, a command-line interface (CLI), and a browser-based platform. The plugin and CLI tool allow engineers to write unit tests more efficiently, while the platform provides unit test visualization and asynchronous unit test generation. We evaluated NxtUnit by generating unit tests for 13 open-source repositories and 500 ByteDance in-house repositories, resulting in a code coverage of 20.74% for in-house repositories. We conducted a survey among ByteDance engineers and found that NxtUnit can save them 48% of the time on writing unit tests. We have made the CLI tool available at https://github.com/bytedance/nxt_unit.","tags":["Software Testing"],"title":"NxtUnit: Automated Unit Test Generation for Go","type":"publication"},{"authors":["Jingling Sun","Ting Su","Kai Liu","Chao Peng","Zhao Zhang","Geguang Pu","Tao Xie","Zhendong Su"],"categories":[],"content":"","date":1671404400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682920984,"objectID":"2c23842d4884856231c85a76f6f09104","permalink":"/publication/tse22/","publishdate":"2019-09-30T00:00:00+01:00","relpermalink":"/publication/tse22/","section":"publication","summary":"Android, the most popular mobile system, offers a number of user-configurable system settings (e.g., network, location, and permission) for controlling devices and apps. Even popular, well-tested apps may fail to properly adapt their behaviors to diverse setting changes, thus frustrating their users. However, there exists no effort to systematically investigate such defects. To this end, we conduct the first large-scale empirical study to understand and characterize these system setting-related defects(in short as “setting defects”), which reside in apps and are triggered by system setting changes. We devote substantial manual effort (over four person-months) to analyze 1,074 setting defects from 180 popular apps on GitHub. We investigate the impact, root causes, and consequences of these setting defects and their correlations. We find that (1) setting defects have a wide impact on apps’ correctness with diverse root causes, (2) the majority of these defects (≈70.7%) cause non-crashing (logic) failures, and (3) some correlations exist between the setting categories, root causes, and consequences. Motivated and informed by these findings, we propose two bug-finding techniques that can synergistically detect setting defects from both the GUI and code levels. Specifically, at the GUI level,we design and introduce setting-wise metamorphic fuzzing, the first automated dynamic testing technique to detect setting defects(causing crash and non-crashing failures, respectively) for Android apps. We implement this technique as an end-to-end, automatedGUI testing tool named SETDROID. At the code level, we distill two major fault patterns and implement a static analysis tool named SETCHECKER to identify potential setting defects. We evaluate SETDROID and SETCHECKER on 26 popular, open-source Android apps, and they find 48 unique, previously-unknown setting defects. To date, 35 have been confirmed and 21 have been fixed by app developers. We also apply SETDROID and SETCHECKER on five highly popular industrial apps, namely WeChat, QQMail, TikTok,CapCut, and Alipay HK, all of which each have billions of monthly active users. SETDROID successfully detects 17 previously unknown setting defects in these apps’ latest releases, and all defects have been confirmed and fixed by the app vendors. After that, we collaborate with ByteDance and deploy these two bug-finding techniques internally to stress-test TikTok, one of its major app products.Within a two-month testing campaign, SETDROID successfully finds 53 setting defects, and SETCHECKER finds 22 ones. So far, 59have been confirmed and 31 have been fixed. All these defects escaped from prior developer testing. By now, SETDROIDhas been integrated into ByteDance’s official app testing infrastructure named FASTBOT for daily testing. These results demonstrate the strong effectiveness and practicality of our proposed techniques.","tags":["Empirical study","System Settings","Android Apps","GUI Testing","Static Analysis"],"title":"Characterizing and Finding System Setting-Related Defects in Android Apps","type":"publication"},{"authors":["Zhengwei Lv","Chao Peng","Zhao Zhang","Ting Su","Kai Liu","Ping Yang"],"categories":[],"content":"","date":1665356400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682920984,"objectID":"ad61cc90df5a40be0fe783def011ea5a","permalink":"/publication/fastbot2/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/fastbot2/","section":"publication","summary":"In the industrial setting, mobile apps undergo frequent updates to catch up with the changing real-world requirements. It leads to the strong practical demands of continuous testing, i.e., obtaining quick feedback on app quality during development. However, existing automated GUI testing techniques fall short in this scenario as they simply run an app version from scratch and do not reuse the knowledge from previous testing runs to accelerate the testing cycle. To fill this important gap, we introduce a reusable automated model-based GUI testing technique. Our key insight is that the knowledge of event-activity transitions from the previous testing runs, i.e., executing which events can reach which activities, is valuable for guiding the follow-up testing runs to quickly cover major app functionalities. To this end, we propose (1) a probabilistic model to memorize and leverage this knowledge during testing, and (2) design a model-based guided testing strategy (enhanced by a reinforcement learning algorithm), to achieve faster-and-higher coverage testing. We implemented our technique as an automated testing tool named Fastbot2. Our evaluation on the two popular industrial apps (with billions of user installations) from ByteDance, Douyin and Toutiao, shows that Fastbot2 outperforms the state-of-the-art testing tools (Monkey, APE and Stoat) in both activity coverage and fault detection in the context of continuous testing. To date, Fastbot2 has been deployed in the CI pipeline at ByteDance for nearly two years, and 50.8% of the developer-fixed crash bugs were reported by Fastbot2, which significantly improves app quality. Fastbot2 has been made publicly available to benefit the community at: https://github.com/bytedance/Fastbot_Android. To date, it has received 500+ stars on GitHub and been used by many app vendors and individual developers to test their apps.","tags":["Software Testing"],"title":"Fastbot2: Reusable Automated Model-based GUI Testing for Android Enhanced by Reinforcement Learning","type":"publication"},{"authors":["Chao Peng","Yujun Gao","Ping Yang"],"categories":[],"content":"","date":1664146800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671527522,"objectID":"9fb860aad0c495dc991a46ae721a3754","permalink":"/publication/sit/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/sit/","section":"publication","summary":"A server API bug could have a huge impact on the operation of other servers and clients relying on that API, resulting in service downtime and financial losses. A common practice of server API testing inside enterprises is writing test inputs and assertions manually, and the test effectiveness depends largely on testers' carefulness, expertise and domain knowledge. Writing test cases for complicated business scenarios with multiple and ordered API calls is also a heavy task that requires a lot of human effort. In this paper, we present the design and deployment of SIT, a fully automated server reliability testing platform at ByteDance that provides capabilities including (1) traffic data generation based on combinatorial testing and fuzzing, (2) scenario testing for complicated business logics and (3) automated test execution with fault localisation in a controlled environment that does not affect online services. SIT has been integrated into the source control system and is triggered when new code change is submitted or configured as scheduled tasks. During the year of 2021, SIT blocked 434 valid issues before they were introduced into the production system.","tags":["Server Testing","Traffic Record and Replay","automated testing"],"title":"Automated Server Testing: an Industrial Experience Report","type":"publication"},{"authors":["Chao Peng","Zhao Zhang","Zhengwei Lv","Ping Yang"],"categories":[],"content":"","date":1664146800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671527522,"objectID":"7aa704b67ad5cd7a5ec1778b665e999e","permalink":"/publication/mubot/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/mubot/","section":"publication","summary":"Automated GUI testing has been playing a key role to uncover crashes to ensure the stability and robustness of Android apps. Recent research has proposed random, search-based and model-based testing techniques for GUI event generation. In industrial practices, different companies have developed various GUI exploration tools such as Facebook Sapienz, WeChat WeTest and ByteDance Fastbot to test their products. However, these tools are bound to their predefined GUI exploration strategies and lack of the ability to generate human-like actions to test meaningful scenarios. To address these challenges, Humanoid is the first Android testing tool that utilises deep learning to imitate human behaviours and achieves promising results over current model-based methods. However, we find some challenges when applying Humanoid to test our sophisticated commercial apps such as infinite loops and low test coverage. To this end, we performed the first case study on the performance of deep learning techniques using commercial apps to understand the underlying reason of the current weakness of this promising method. Based on our findings, we propose MUBot (Multi-modal User Bot) for human-like Android testing. Our empirical evaluation reveals that MUBot has better performance over Humanoid and Fastbot, our in-house testing tool on coverage achieved and bug-fixing rate on commercial apps.","tags":["Android Testing","Graphical User Interface","Deep Learning"],"title":"MUBot: Learning to Test Large-Scale Commercial Android Apps like a Human","type":"publication"},{"authors":["Sefa Akca","Chao Peng","Ajitha Rajan"],"categories":[],"content":"","date":1633906800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629115120,"objectID":"33bf610efa2b83ae8f95d6d9ede0c9b5","permalink":"/publication/esem21/","publishdate":"2021-06-30T00:00:00+01:00","relpermalink":"/publication/esem21/","section":"publication","summary":"Executing, verifying and enforcing credible transactions on permissionless blockchains is done using smart contracts. A key challenge with smart contracts is ensuring their correctness and security. Several test input generation techniques for detecting vulnerabilities in smart contracts have been proposed in the last few years. However,a comparison of proposed techniques to gauge their effectivenessis missing. This paper conducts an empirical evaluation of testing techniques for smart contracts. The testing techniques we evaluated are: (1) Blackbox fuzzing, (2) Adaptive fuzzing, (3) Coverage-guided fuzzing with an SMT solver and (4) Genetic algorithm. We do not consider static analysis tools, as several recent studies have assessed and compared effectiveness of these tools. We evaluate effectiveness of the test generation techniques using (1) Coverage achieved - we use four code coverage metrics targeting smart contracts, (2) Fault finding ability - using aritificially seeded and real security vulnerabilities of different types. We used two datasets in our evaluation - one with 1665 real smart contracts from Etherscan, and another with 90 real contracts with known vulnerabilities to assess fault finding ability. We find Adaptive fuzzing performs best in terms of coverage and fault finding over contractsin both datasets.","tags":["Ethereum","Smart Contract","Fault Seeding","Test Generation","Genetic Algorithm","Fuzzing","SMT Solving","Software Testing"],"title":"Testing Smart Contracts: Which Technique Performs Best?","type":"publication"},{"authors":["Chao Peng","Ajitha Rajan","Tianqin Cai"],"categories":[],"content":"","date":1632697200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629115120,"objectID":"8f8a9e22eb25d3f7dbf0d0960b503dbd","permalink":"/publication/cat/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/cat/","section":"publication","summary":"Android Apps are frequently updated, every couple of weeks, to keep up with changing user, hardware and business demands. Correctness of App updates is checked through extensive testing. Recent research has proposed tools for automated GUI event generation in Android Apps. These techniques, however, are not efficient at checking App updates as the generated GUI events do not prioritise updates, and instead explore other App behaviours. We address this need in this paper with CAT (Change-focused Android GUI Testing). For App updates, at the source code or GUI level, CAT performs change impact analysis to identify GUI elements affected by the update. CAT then generates GUI event sequences to interact with the affected GUI elements. Our empirical evaluations using 21 publicly available open source and 2 commercial Android Apps demonstrate that CAT is able to automatically identify GUI elements affected by App updates, generate and execute GUI event sequences focusing on change-affected GUI elements. Comparison with two popular GUI event generation tools, DroidBot and DroidMate, revealed that CAT was more effective at interacting with the change-affected GUI elements. Finally, CAT was able to detect previously unknown change-related bugs in two open source Apps. Developers of the commercial Apps found CAT was more effective than their in-house GUI testing tool in interacting with changed elements and faster at detecting seeded bugs.","tags":["Software Testing","Android","Graphical User Interface","Program Analysis"],"title":"CAT: Change-focused Android GUI Testing","type":"publication"},{"authors":["Chao Peng","Ajitha Rajan"],"categories":[],"content":"","date":1582412400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597180930,"objectID":"49cee828c32f6e70e9459b592eea2ca5","permalink":"/publication/gpgpu/","publishdate":"2020-02-23T00:00:00+01:00","relpermalink":"/publication/gpgpu/","section":"publication","summary":"Graphics Processing Units (GPUs) are massively parallel processors offering performance acceleration and energy efficiency unmatched by current processors (CPUs) in computers. These advantages along with recent advances in the programmability of GPUs have made them attractive for general-purpose computations. Despite the advances in programmability, GPU kernels are hard to code and analyse due to the high complexity of memory sharing patterns, striding patterns for memory accesses, implicit synchronisation, and combinatorial explosion of thread interleavings. Existing few techniques for testing GPU kernels use symbolic execution for test generation that incur a high overhead, have limited scalability and do not handle all data types.\n\nWe propose a test generation technique for OpenCL kernels that combines mutation-based fuzzing and selective constraint solving with the goal of being fast, effective and scalable. Fuzz testing for GPU kernels has not been explored previously. Our approach for fuzz testing randomly mutates input kernel argument values with the goal of increasing branch coverage. When fuzz testing is unable to increase branch coverage with random mutations, we gather path constraints for uncovered branch conditions and invoke the Z3 constraint solver to generate tests for them.\n\nIn addition to the test generator, we also present a schedule amplifier that simulates multiple work-group schedules, with which to execute each of the generated tests. The schedule amplifier is designed to help uncover inter work-group data races. We evaluate the effectiveness of the generated tests and schedule amplifier using 217 kernels from open source projects and industry standard benchmark suites measuring branch coverage and fault finding. We find our test generation technique achieves close to 100% coverage and mutation score for majority of the kernels. Overhead incurred in test generation is small (average of 0.8 seconds). We also confirmed our technique scales easily to large kernels, and can support all OpenCL data types, including complex data structures.","tags":["Software Testing","GPU","OpenCL","Test Case Generation","Fuzz Testing","Constraint Solving","Data Race"],"title":"Automated Test Generation for OpenCL Kernels Using Fuzzing and Constraint Solving","type":"publication"},{"authors":["Chao Peng","Sefa Akca","Ajitha Rajan"],"categories":[],"content":"","date":1569798000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682920984,"objectID":"9f921ef3705e0d431e799e2f871b590d","permalink":"/publication/sif/","publishdate":"2019-09-30T00:00:00+01:00","relpermalink":"/publication/sif/","section":"publication","summary":"Solidity is an object-oriented and high-level language for writing smart contracts that are used to execute, verify and enforce credible transactions on permissionless blockchains. In the last few years, analysis of smart contracts has raised considerable interest and numerous techniques have been proposed to check the presence of vulnerabilities in them. Current techniques lack traceability in source code and have widely differing work flows. There is no single unifying framework for analysis, instrumentation, optimisation and code generation of Solidity contracts.\n\nIn this paper, we present SIF, a comprehensive framework for Solidity contract analysis, query, instrumentation, and code generation. SIF provides support for Solidity contract developers and testers to build source level techniques for analysis, understanding, diagnostics, optimisations and code generation. We show feasibility and applicability of the framework by building practical tools on top of it and running them on 1838 real smart contracts deployed on the Ethereum network.","tags":["High Level Languages","Software Testing","Code Instrumentation","Program Analysis"],"title":"SIF: A Framework for Solidity Contract Instrumentation and Analysis","type":"publication"},{"authors":["Sefa Akca","Ajitha Rajan","Chao Peng"],"categories":[],"content":"","date":1569798000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624345887,"objectID":"f10cb3c35c72a89ffcbc0104521f9ee9","permalink":"/publication/solanalyser/","publishdate":"2019-09-30T00:00:00+01:00","relpermalink":"/publication/solanalyser/","section":"publication","summary":"Executing, verifying and enforcing credible transactions on permissionless blockchains is done using smart contracts. Smart contracts define and execute crucial agreements, and attacks exploiting their vulnerabilities can lead to huge losses, like the failure of the DAO smart contract that resulted in 50 million US Dollars worth of losses. A key challenge with smart contracts is ensuring their correctness and security.\n\nTo address this challenge, we present a fully automated technique, SolAnalyser, for vulnerability detection over Solidity smart contracts that uses both static and dynamic analysis. Analysis techniques for smart contracts in the literature rely on static analysis with a high rate of false positives or lack support for vulnerabilities like out of gas, unchecked send, timestamp dependency. We instrument the original smart contract with property checks and use dynamic analysis to tackle this problem. Our tool, SolAnalyser, supports automated detection of 8 different vulnerability types that currently lack wide support in existing tools, and can easily be extended to support other types. We also implemented a fault seeding tool that injects different types of vulnerabilities in smart contracts. We use the mutated contracts for assessing the effectiveness of different analysis tools.\n\nOur experiment uses 1838 real contracts from which we generate 12866 mutated contracts by artificially seeding 8 different vulnerability types. We evaluate the effectiveness of our technique in revealing the seeded vulnerabilities and compare against five existing popular analysis tools – Oyente, Securify, Maian, SmartCheck and Mythril. This is the first large scale evaluation of existing tools that compares their effectiveness by running them on a common set of contracts. We find that our technique outperforms all five existing tools in supporting detection of all 8 vulnerability types and in achieving higher precision and recall rate. SolAnalyser was also faster in analysing the different vulnerabilities than any of the existing tools in our experiment. Among existing tools, Securify achieved high precision in detecting integer overflow, underflow, and division by zero but had poor recall rates.","tags":["Blockchain","Smart Contract","Testing","Static Analysis","Assertions","Fault Seeding"],"title":"SolAnalyser: A Framework for Analysing and Testing Smart Contracts","type":"publication"},{"authors":["Chao Peng"],"categories":[],"content":"","date":1559170800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597180930,"objectID":"bb05b16b729dbb9d601d7df746c67617","permalink":"/publication/isstads/","publishdate":"2019-05-30T00:00:00+01:00","relpermalink":"/publication/isstads/","section":"publication","summary":"Testing is an important and challenging part of software development and its effectiveness depends on the quality of test cases. However, there exists no means of measuring quality of tests developed for GPU programs and as a result, no test case generation techniques for GPU programs aiming at high test effectiveness. Existing criteria for sequential and threaded CPU programs cannot be directly applied to GPU programs as GPU follows a completely different memory and execution model.\n\nWe surveyed existing work on GPU program verification and bug fixes of open source GPU programs. Based on our findings, we define barrier, branch and loop coverage criteria and propose a set of mutation operators to measure fault finding capabilities of test cases. CLTestCheck, a framework for measuring quality of tests developed for GPU programs by code coverage analysis, fault seeding and work-group schedule amplification has been developed and evaluated using industry standard benchmarks. Experiments show that the framework is able to automatically measure test effectiveness and reveal unusual behaviours. Our planned work includes data flow coverage adopted for GPU programs to probe the underlying cause of unusual kernel behaviours and a more comprehensive work-group scheduler. We also plan to design and develop an automatic test case generator aiming at generating high quality test suites for GPU programs.","tags":["Software Testing","Code Coverage","Fault Finding","Data Race","GPU","OpenCL","Test Case Generation"],"title":"On the Correctness of GPU Programs","type":"publication"},{"authors":["Chao Peng","Ajitha Rajan"],"categories":[],"content":"","date":1554505200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682920984,"objectID":"7e0be6aad3b6512a7eefe822ae06aaae","permalink":"/publication/cltestcheck/","publishdate":"2019-04-06T00:00:00+01:00","relpermalink":"/publication/cltestcheck/","section":"publication","summary":"Massive parallelism, and energy efficiency of GPUs, along with advances in their programmability with OpenCL and CUDA programming models have made them attractive for general-purpose computations across many application domains. Techniques for testing GPU kernels have emerged recently to aid the construction of correct GPU software. However, there exists no means of measuring quality and effectiveness of tests developed for GPU kernels. Traditional coverage criteria over CPU programs is not adequate over GPU kernels as it uses a completely different programming model and the faults encountered may be specific to the GPU architecture. GPUs have SIMT (single instruction, multiple thread) execution model that executes batches of threads (work groups) in lock-step, i.e all threads in a work group execute the same instruction but on different data.\n\nWe address this need in this paper and present a framework, CLTestCheck, for assessing quality of test suites developed for OpenCL kernels. The framework has the following capabilities, 1. Measures kernel code coverage using three different coverage metrics that are inspired by faults found in real kernel code, 2. Seeds different types of faults in kernel code and measures fault finding capability of test suite, 3. Simulates different work group schedules to check for potential data races with the given test suite. We conducted empirical evaluation of CLTestCheck on a collection of 82 publicly available GPU kernels and test suites. We found that CLTestCheck is capable of automatically measuring effectiveness of test suites, in terms of kernel code coverage, fault finding and revealing data races in real OpenCL kernels.","tags":["Testing","Code Coverage","Fault Finding","Data Race","Mutation Testing","GPU","OpenCL"],"title":"CLTestCheck: Measuring Test Effectiveness for GPU Kernels","type":"publication"}]