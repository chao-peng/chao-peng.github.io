[{"authors":["admin"],"categories":null,"content":"Chao Peng （彭péng, 超chāo） is a PhD candidate at the Laboratory for Foundations of Computer Science (LFCS), School of Informatics, The University of Edinburgh. He holds an MSc in High Performance Computing with Data Science from the University of Edinburgh and a BEng in Computer Science and Technology from Xuzhou University of Technology. During his MSc, he was a member of Team EPCC for the International Supercomputing Conference Student Cluster Competition.\nHis research interest lies in the area of massively parallel architectures and programming. He is currently doing research in defining code coverage metrics for GPU programs and automated test case generation, reduction and execution. His supervisor is Dr. Ajitha Rajan.\nEmail:\ndomain=\u0026quot;ed.ac.uk\u0026quot; email=\u0026quot;chao.peng@${domain}\u0026quot;  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1605102922,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/chao-peng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chao-peng/","section":"authors","summary":"Chao Peng （彭péng, 超chāo） is a PhD candidate at the Laboratory for Foundations of Computer Science (LFCS), School of","tags":null,"title":"Chao Peng","type":"authors"},{"authors":["Chao Peng","Ajitha Rajan"],"categories":[],"content":"","date":1582412400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597180930,"objectID":"49cee828c32f6e70e9459b592eea2ca5","permalink":"/publication/gpgpu/","publishdate":"2020-02-23T00:00:00+01:00","relpermalink":"/publication/gpgpu/","section":"publication","summary":"Graphics Processing Units (GPUs) are massively parallel processors offering performance acceleration and energy efficiency unmatched by current processors (CPUs) in computers. These advantages along with recent advances in the programmability of GPUs have made them attractive for general-purpose computations. Despite the advances in programmability, GPU kernels are hard to code and analyse due to the high complexity of memory sharing patterns, striding patterns for memory accesses, implicit synchronisation, and combinatorial explosion of thread interleavings. Existing few techniques for testing GPU kernels use symbolic execution for test generation that incur a high overhead, have limited scalability and do not handle all data types.\n\nWe propose a test generation technique for OpenCL kernels that combines mutation-based fuzzing and selective constraint solving with the goal of being fast, effective and scalable. Fuzz testing for GPU kernels has not been explored previously. Our approach for fuzz testing randomly mutates input kernel argument values with the goal of increasing branch coverage. When fuzz testing is unable to increase branch coverage with random mutations, we gather path constraints for uncovered branch conditions and invoke the Z3 constraint solver to generate tests for them.\n\nIn addition to the test generator, we also present a schedule amplifier that simulates multiple work-group schedules, with which to execute each of the generated tests. The schedule amplifier is designed to help uncover inter work-group data races. We evaluate the effectiveness of the generated tests and schedule amplifier using 217 kernels from open source projects and industry standard benchmark suites measuring branch coverage and fault finding. We find our test generation technique achieves close to 100% coverage and mutation score for majority of the kernels. Overhead incurred in test generation is small (average of 0.8 seconds). We also confirmed our technique scales easily to large kernels, and can support all OpenCL data types, including complex data structures.","tags":["Software Testing","GPU","OpenCL","Test Case Generation","Fuzz Testing","Constraint Solving","Data Race"],"title":"Automated Test Generation for OpenCL Kernels Using Fuzzing and Constraint Solving","type":"publication"},{"authors":["Chao Peng","Sefa Akca","Ajitha Rajan"],"categories":[],"content":"","date":1569798000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597180930,"objectID":"9f921ef3705e0d431e799e2f871b590d","permalink":"/publication/sif/","publishdate":"2019-09-30T00:00:00+01:00","relpermalink":"/publication/sif/","section":"publication","summary":"Solidity is an object-oriented and high-level language for writing smart contracts that are used to execute, verify and enforce credible transactions on permissionless blockchains. In the last few years, analysis of smart contracts has raised considerable interest and numerous techniques have been proposed to check the presence of vulnerabilities in them. Current techniques lack traceability in source code and have widely differing work flows. There is no single unifying framework for analysis, instrumentation, optimisation and code generation of Solidity contracts.\n\nIn this paper, we present SIF, a comprehensive framework for Solidity contract analysis, query, instrumentation, and code generation. SIF provides support for Solidity contract developers and testers to build source level techniques for analysis, understanding, diagnostics, optimisations and code generation. We show feasibility and applicability of the framework by building practical tools on top of it and running them on 1838 real smart contracts deployed on the Ethereum network.","tags":["high level languages","software testing","code instrumentation","program analysis"],"title":"SIF: A Framework for Solidity Contract Instrumentation and Analysis","type":"publication"},{"authors":["Sefa Akca","Ajitha Rajan","Chao Peng"],"categories":[],"content":"","date":1569798000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597180930,"objectID":"f10cb3c35c72a89ffcbc0104521f9ee9","permalink":"/publication/solanalyser/","publishdate":"2019-09-30T00:00:00+01:00","relpermalink":"/publication/solanalyser/","section":"publication","summary":"Executing, verifying and enforcing credible transactions on permissionless blockchains is done using smart contracts. Smart contracts define and execute crucial agreements, and attacks exploiting their vulnerabilities can lead to huge losses, like the failure of the DAO smart contract that resulted in 50 million US Dollars worth of losses. A key challenge with smart contracts is ensuring their correctness and security.\n\nTo address this challenge, we present a fully automated technique, SolAnalyser, for vulnerability detection over Solidity smart contracts that uses both static and dynamic analysis. Analysis techniques for smart contracts in the literature rely on static analysis with a high rate of false positives or lack support for vulnerabilities like out of gas, unchecked send, timestamp dependency. We instrument the original smart contract with property checks and use dynamic analysis to tackle this problem. Our tool, SolAnalyser, supports automated detection of 8 different vulnerability types that currently lack wide support in existing tools, and can easily be extended to support other types. We also implemented a fault seeding tool that injects different types of vulnerabilities in smart contracts. We use the mutated contracts for assessing the effectiveness of different analysis tools.\n\nOur experiment uses 1838 real contracts from which we generate 12866 mutated contracts by artificially seeding 8 different vulnerability types. We evaluate the effectiveness of our technique in revealing the seeded vulnerabilities and compare against five existing popular analysis tools – Oyente, Securify, Maian, SmartCheck and Mythril. This is the first large scale evaluation of existing tools that compares their effectiveness by running them on a common set of contracts. We find that our technique outperforms all five existing tools in supporting detection of all 8 vulnerability types and in achieving higher precision and recall rate. SolAnalyser was also faster in analysing the different vulnerabilities than any of the existing tools in our experiment. Among existing tools, Securify achieved high precision in detecting integer overflow, underflow, and division by zero but had poor recall rates.","tags":["blockchain","smart contract","testing","static analysis","assertions","fault seeding"],"title":"SolAnalyser: A Framework for Analysing and Testing Smart Contracts","type":"publication"},{"authors":["Chao Peng"],"categories":[],"content":"","date":1559170800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597180930,"objectID":"bb05b16b729dbb9d601d7df746c67617","permalink":"/publication/isstads/","publishdate":"2019-05-30T00:00:00+01:00","relpermalink":"/publication/isstads/","section":"publication","summary":"Testing is an important and challenging part of software development and its effectiveness depends on the quality of test cases. However, there exists no means of measuring quality of tests developed for GPU programs and as a result, no test case generation techniques for GPU programs aiming at high test effectiveness. Existing criteria for sequential and threaded CPU programs cannot be directly applied to GPU programs as GPU follows a completely different memory and execution model.\n\nWe surveyed existing work on GPU program verification and bug fixes of open source GPU programs. Based on our findings, we define barrier, branch and loop coverage criteria and propose a set of mutation operators to measure fault finding capabilities of test cases. CLTestCheck, a framework for measuring quality of tests developed for GPU programs by code coverage analysis, fault seeding and work-group schedule amplification has been developed and evaluated using industry standard benchmarks. Experiments show that the framework is able to automatically measure test effectiveness and reveal unusual behaviours. Our planned work includes data flow coverage adopted for GPU programs to probe the underlying cause of unusual kernel behaviours and a more comprehensive work-group scheduler. We also plan to design and develop an automatic test case generator aiming at generating high quality test suites for GPU programs.","tags":["Software Testing","Code Coverage","Fault Finding","Data Race","GPU","OpenCL","Test Case Generation"],"title":"On the Correctness of GPU Programs","type":"publication"},{"authors":["Chao Peng","Ajitha Rajan"],"categories":[],"content":"","date":1554505200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597180930,"objectID":"7e0be6aad3b6512a7eefe822ae06aaae","permalink":"/publication/cltestcheck/","publishdate":"2019-04-06T00:00:00+01:00","relpermalink":"/publication/cltestcheck/","section":"publication","summary":"Massive parallelism, and energy efficiency of GPUs, along with advances in their programmability with OpenCL and CUDA programming models have made them attractive for general-purpose computations across many application domains. Techniques for testing GPU kernels have emerged recently to aid the construction of correct GPU software. However, there exists no means of measuring quality and effectiveness of tests developed for GPU kernels. Traditional coverage criteria over CPU programs is not adequate over GPU kernels as it uses a completely different programming model and the faults encountered may be specific to the GPU architecture. GPUs have SIMT (single instruction, multiple thread) execution model that executes batches of threads (work groups) in lock-step, i.e all threads in a work group execute the same instruction but on different data.\n\nWe address this need in this paper and present a framework, CLTestCheck, for assessing quality of test suites developed for OpenCL kernels. The framework has the following capabilities, 1. Measures kernel code coverage using three different coverage metrics that are inspired by faults found in real kernel code, 2. Seeds different types of faults in kernel code and measures fault finding capability of test suite, 3. Simulates different work group schedules to check for potential data races with the given test suite. We conducted empirical evaluation of CLTestCheck on a collection of 82 publicly available GPU kernels and test suites. We found that CLTestCheck is capable of automatically measuring effectiveness of test suites, in terms of kernel code coverage, fault finding and revealing data races in real OpenCL kernels.","tags":["Testing","Code Coverage","Fault Finding","Data Race","Mutation Testing","GPU","OpenCL"],"title":"CLTestCheck: Measuring Test Effectiveness for GPU Kernels","type":"publication"}]