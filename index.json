[{"authors":["admin"],"categories":null,"content":"Chao Peng （彭超） is a PhD candidate at the Laboratory for Foundations of Computer Science (LFCS), School of Informatics, The University of Edinburgh. He holds an MSc in High Performance Computing with Data Science from the University of Edinburgh and a BEng in Computer Science and Technology from Xuzhou University of Technology. During his MSc, he was a member of Team EPCC for the International Supercomputing Conference Student Cluster Competition.\nHis research interest lies in the area of massively parallel architectures and programming. He is currently doing research in defining code coverage metrics for GPU programs and automated test case generation, reduction and execution. His supervisor is Dr. Ajitha Rajan.\nEmail: ku.ca.de@gnep.oahc\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1550760045,"objectID":"3eef613d86e546bf0ce1d3fb271035e3","permalink":"https://chao-peng.github.io/author/chao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chao/","section":"author","summary":"Chao Peng （彭超） is a PhD candidate at the Laboratory for Foundations of Computer Science (LFCS), School of Informatics, The University of Edinburgh. He holds an MSc in High Performance Computing with Data Science from the University of Edinburgh and a BEng in Computer Science and Technology from Xuzhou University of Technology. During his MSc, he was a member of Team EPCC for the International Supercomputing Conference","tags":null,"title":"Chao Peng","type":"author"},{"authors":["Chao Peng"],"categories":null,"content":"","date":1559170800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559231172,"objectID":"bb05b16b729dbb9d601d7df746c67617","permalink":"https://chao-peng.github.io/publication/isstads/","publishdate":"2019-05-30T00:00:00+01:00","relpermalink":"/publication/isstads/","section":"publication","summary":"Testing is an important and challenging part of software development and its effectiveness depends on the quality of test cases. However, there exists no means of measuring quality of tests developed for GPU programs and as a result, no test case generation techniques for GPU programs aiming at high test effectiveness. Existing criteria for sequential and threaded CPU programs cannot be directly applied to GPU programs as GPU follows a completely different memory and\nexecution model. \n\nWe surveyed existing work on GPU program verification and bug fixes of open source GPU programs. Based on our findings, we define barrier, branch and loop coverage criteria and propose a set of mutation operators to measure fault finding capabilities of test cases. CLTestCheck, a framework for measuring quality of tests developed for GPU programs by code coverage analysis, fault seeding and work-group schedule amplification has been developed and evaluated using industry\nstandard benchmarks. Experiments show that the framework is able to automatically measure test effectiveness and reveal unusual behaviours. Our planned work includes data flow coverage adopted for GPU programs to probe the underlying cause of unusual kernel behaviours and a more comprehensive work-group scheduler. We also plan to design and develop an automatic test case generator aiming at generating high quality test suites for GPU programs.\n","tags":[],"title":"On the Correctness of GPU Programs","type":"publication"},{"authors":["Chao Peng","Ajitha Rajan"],"categories":null,"content":"","date":1556924400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557007893,"objectID":"9f921ef3705e0d431e799e2f871b590d","permalink":"https://chao-peng.github.io/publication/sif/","publishdate":"2019-05-04T00:00:00+01:00","relpermalink":"/publication/sif/","section":"publication","summary":"Solidity is an object-oriented and high-level language for writing smart contracts which are used to execute, verify and enforce credible transactions on permissionless blockchains. In the last few years, analysis of vulnerabilities in smart contracts has raised considerable interest and numerous techniques have been proposed. Current techniques lack traceability in source code and have widely differing work flows. There is no single unifying framework for analysis, instrumentation, optimisation and code generation of Solidity contracts.\n\nIn this paper, we present SIF, a comprehensive framework for Solidity contract monitoring, instrumenting, and code generation. SIF provides support for Solidity contract developers and testers to build source level techniques for analysis, bug detection, coverage measurement, optimisations and code generation. We show feasibility and applicability of the framework using 51 real smart contracts deployed on the Ethereum network.\n","tags":[],"title":"SIF: A Framework for Solidity Code Instrumentation and Analysis","type":"publication"},{"authors":["Chao Peng","Ajitha Rajan"],"categories":null,"content":"","date":1554505200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557007358,"objectID":"7e0be6aad3b6512a7eefe822ae06aaae","permalink":"https://chao-peng.github.io/publication/cltestcheck/","publishdate":"2019-04-06T00:00:00+01:00","relpermalink":"/publication/cltestcheck/","section":"publication","summary":"Massive parallelism, and energy efficiency of GPUs, along with \nadvances in their programmability with OpenCL and CUDA programming models have made them attractive for general-purpose computations across many application domains.\nTechniques for testing GPU kernels have emerged recently to aid the construction of correct GPU software. \nHowever, there exists no means of measuring quality and effectiveness of tests developed for GPU kernels. \nTraditional coverage criteria over CPU programs is not adequate over GPU kernels as it uses a completely different programming model and the faults encountered may be  specific to the GPU architecture. \nGPUs have SIMT (single instruction, multiple thread) execution model that executes batches of threads (work groups) in lock-step, i.e all threads in a work group execute the same instruction but on different data.  \n\nWe address this need in this paper and present a framework, CLTestCheck, for assessing quality of test suites developed for OpenCL kernels. The framework has the following capabilities, \n1. Measures kernel code coverage using three different coverage metrics that are inspired by faults found in real kernel code, 2. Seeds different types of faults in kernel code and measures fault finding capability of test suite, 3. Simulates different work group schedules to check for potential data races with the given test suite. We conducted empirical evaluation of CLTestCheck on a collection of 82 publicly available GPU kernels and test suites. We found that CLTestCheck is capable of automatically measuring effectiveness of test suites, in terms of kernel code coverage, fault finding and revealing data races in real OpenCL kernels. \n","tags":[],"title":"CLTestCheck: Measuring Test Effectiveness for GPU Kernels","type":"publication"}]