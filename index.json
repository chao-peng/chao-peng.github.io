[{"authors":["admin"],"categories":null,"content":"Chao Peng （彭péng, 超chāo）\nI am a Senior Researcher at ByteDance (字节跳动) and a part-time postgraduate student mentor of School of Computer Science, Fudan University. I received my PhD degree from Laboratory for Foundations of Computer Science (LFCS), The University of Edinburgh under supervision of Dr. Ajitha Rajan.\nAt ByteDance, I am on the App Infra Research Center. My research interest lies in the area of software testing, program repair and optimisations, and the synergy with machine learning and compiler techniques. I am also responsible for department-wide academic development and university collaboration.\nI am passionate about building practical software testing, analysis, and debugging systems to predict, detect, diagnose, and fix bugs for all kinds of software systems.\nOutside of work, I enjoy going to the gym.\nEmail:\nprefix=\u0026quot;pengchao.x\u0026quot; company=\u0026quot;bytedance\u0026quot; email=${prefix}@${company}.com  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1652855409,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/chao-peng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chao-peng/","section":"authors","summary":"Chao Peng （彭péng, 超chāo） I am a Senior Researcher at ByteDance (字节跳动) and a","tags":null,"title":"Chao Peng","type":"authors"},{"authors":["Chao Peng","Yujun Gao","Ping Yang"],"categories":[],"content":"","date":1664146800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655885853,"objectID":"9fb860aad0c495dc991a46ae721a3754","permalink":"/publication/sit/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/sit/","section":"publication","summary":"To be added.","tags":["Software Testing"],"title":"Automated Server Testing: an Industrial Experience Report","type":"publication"},{"authors":["Chao Peng","Zhao Zhang","Zhengwei Lv","Ping Yang"],"categories":[],"content":"","date":1664146800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655885853,"objectID":"7aa704b67ad5cd7a5ec1778b665e999e","permalink":"/publication/mubot/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/mubot/","section":"publication","summary":"To be added","tags":["Software Testing","Android","Graphical User Interface","Program Analysis"],"title":"MUBot: Learning to Test Large-Scale Commercial Android Apps like a Human","type":"publication"},{"authors":["Sefa Akca","Chao Peng","Ajitha Rajan"],"categories":[],"content":"","date":1633906800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629115120,"objectID":"33bf610efa2b83ae8f95d6d9ede0c9b5","permalink":"/publication/esem21/","publishdate":"2021-06-30T00:00:00+01:00","relpermalink":"/publication/esem21/","section":"publication","summary":"Executing, verifying and enforcing credible transactions on permissionless blockchains is done using smart contracts. A key challenge with smart contracts is ensuring their correctness and security. Several test input generation techniques for detecting vulnerabilities in smart contracts have been proposed in the last few years. However,a comparison of proposed techniques to gauge their effectivenessis missing. This paper conducts an empirical evaluation of testing techniques for smart contracts. The testing techniques we evaluated are: (1) Blackbox fuzzing, (2) Adaptive fuzzing, (3) Coverage-guided fuzzing with an SMT solver and (4) Genetic algorithm. We do not consider static analysis tools, as several recent studies have assessed and compared effectiveness of these tools. We evaluate effectiveness of the test generation techniques using (1) Coverage achieved - we use four code coverage metrics targeting smart contracts, (2) Fault finding ability - using aritificially seeded and real security vulnerabilities of different types. We used two datasets in our evaluation - one with 1665 real smart contracts from Etherscan, and another with 90 real contracts with known vulnerabilities to assess fault finding ability. We find Adaptive fuzzing performs best in terms of coverage and fault finding over contractsin both datasets.","tags":["Ethereum","Smart Contract","Fault Seeding","Test Generation","Genetic Algorithm","Fuzzing","SMT Solving","Software Testing"],"title":"Testing Smart Contracts: Which Technique Performs Best?","type":"publication"},{"authors":["Chao Peng","Ajitha Rajan","Tianqin Cai"],"categories":[],"content":"","date":1632697200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629115120,"objectID":"8f8a9e22eb25d3f7dbf0d0960b503dbd","permalink":"/publication/cat/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/cat/","section":"publication","summary":"Android Apps are frequently updated, every couple of weeks, to keep up with changing user, hardware and business demands. Correctness of App updates is checked through extensive testing. Recent research has proposed tools for automated GUI event generation in Android Apps. These techniques, however, are not efficient at checking App updates as the generated GUI events do not prioritise updates, and instead explore other App behaviours. We address this need in this paper with CAT (Change-focused Android GUI Testing). For App updates, at the source code or GUI level, CAT performs change impact analysis to identify GUI elements affected by the update. CAT then generates GUI event sequences to interact with the affected GUI elements. Our empirical evaluations using 21 publicly available open source and 2 commercial Android Apps demonstrate that CAT is able to automatically identify GUI elements affected by App updates, generate and execute GUI event sequences focusing on change-affected GUI elements. Comparison with two popular GUI event generation tools, DroidBot and DroidMate, revealed that CAT was more effective at interacting with the change-affected GUI elements. Finally, CAT was able to detect previously unknown change-related bugs in two open source Apps. Developers of the commercial Apps found CAT was more effective than their in-house GUI testing tool in interacting with changed elements and faster at detecting seeded bugs.","tags":["Software Testing","Android","Graphical User Interface","Program Analysis"],"title":"CAT: Change-focused Android GUI Testing","type":"publication"},{"authors":["Chao Peng","Ajitha Rajan"],"categories":[],"content":"","date":1582412400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597180930,"objectID":"49cee828c32f6e70e9459b592eea2ca5","permalink":"/publication/gpgpu/","publishdate":"2020-02-23T00:00:00+01:00","relpermalink":"/publication/gpgpu/","section":"publication","summary":"Graphics Processing Units (GPUs) are massively parallel processors offering performance acceleration and energy efficiency unmatched by current processors (CPUs) in computers. These advantages along with recent advances in the programmability of GPUs have made them attractive for general-purpose computations. Despite the advances in programmability, GPU kernels are hard to code and analyse due to the high complexity of memory sharing patterns, striding patterns for memory accesses, implicit synchronisation, and combinatorial explosion of thread interleavings. Existing few techniques for testing GPU kernels use symbolic execution for test generation that incur a high overhead, have limited scalability and do not handle all data types.\n\nWe propose a test generation technique for OpenCL kernels that combines mutation-based fuzzing and selective constraint solving with the goal of being fast, effective and scalable. Fuzz testing for GPU kernels has not been explored previously. Our approach for fuzz testing randomly mutates input kernel argument values with the goal of increasing branch coverage. When fuzz testing is unable to increase branch coverage with random mutations, we gather path constraints for uncovered branch conditions and invoke the Z3 constraint solver to generate tests for them.\n\nIn addition to the test generator, we also present a schedule amplifier that simulates multiple work-group schedules, with which to execute each of the generated tests. The schedule amplifier is designed to help uncover inter work-group data races. We evaluate the effectiveness of the generated tests and schedule amplifier using 217 kernels from open source projects and industry standard benchmark suites measuring branch coverage and fault finding. We find our test generation technique achieves close to 100% coverage and mutation score for majority of the kernels. Overhead incurred in test generation is small (average of 0.8 seconds). We also confirmed our technique scales easily to large kernels, and can support all OpenCL data types, including complex data structures.","tags":["Software Testing","GPU","OpenCL","Test Case Generation","Fuzz Testing","Constraint Solving","Data Race"],"title":"Automated Test Generation for OpenCL Kernels Using Fuzzing and Constraint Solving","type":"publication"},{"authors":["Chao Peng","Sefa Akca","Ajitha Rajan"],"categories":[],"content":"","date":1569798000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624345887,"objectID":"9f921ef3705e0d431e799e2f871b590d","permalink":"/publication/sif/","publishdate":"2019-09-30T00:00:00+01:00","relpermalink":"/publication/sif/","section":"publication","summary":"Solidity is an object-oriented and high-level language for writing smart contracts that are used to execute, verify and enforce credible transactions on permissionless blockchains. In the last few years, analysis of smart contracts has raised considerable interest and numerous techniques have been proposed to check the presence of vulnerabilities in them. Current techniques lack traceability in source code and have widely differing work flows. There is no single unifying framework for analysis, instrumentation, optimisation and code generation of Solidity contracts.\n\nIn this paper, we present SIF, a comprehensive framework for Solidity contract analysis, query, instrumentation, and code generation. SIF provides support for Solidity contract developers and testers to build source level techniques for analysis, understanding, diagnostics, optimisations and code generation. We show feasibility and applicability of the framework by building practical tools on top of it and running them on 1838 real smart contracts deployed on the Ethereum network.","tags":["High Level Languages","Software Testing","Code Instrumentation","Program Analysis"],"title":"SIF: A Framework for Solidity Contract Instrumentation and Analysis","type":"publication"},{"authors":["Sefa Akca","Ajitha Rajan","Chao Peng"],"categories":[],"content":"","date":1569798000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624345887,"objectID":"f10cb3c35c72a89ffcbc0104521f9ee9","permalink":"/publication/solanalyser/","publishdate":"2019-09-30T00:00:00+01:00","relpermalink":"/publication/solanalyser/","section":"publication","summary":"Executing, verifying and enforcing credible transactions on permissionless blockchains is done using smart contracts. Smart contracts define and execute crucial agreements, and attacks exploiting their vulnerabilities can lead to huge losses, like the failure of the DAO smart contract that resulted in 50 million US Dollars worth of losses. A key challenge with smart contracts is ensuring their correctness and security.\n\nTo address this challenge, we present a fully automated technique, SolAnalyser, for vulnerability detection over Solidity smart contracts that uses both static and dynamic analysis. Analysis techniques for smart contracts in the literature rely on static analysis with a high rate of false positives or lack support for vulnerabilities like out of gas, unchecked send, timestamp dependency. We instrument the original smart contract with property checks and use dynamic analysis to tackle this problem. Our tool, SolAnalyser, supports automated detection of 8 different vulnerability types that currently lack wide support in existing tools, and can easily be extended to support other types. We also implemented a fault seeding tool that injects different types of vulnerabilities in smart contracts. We use the mutated contracts for assessing the effectiveness of different analysis tools.\n\nOur experiment uses 1838 real contracts from which we generate 12866 mutated contracts by artificially seeding 8 different vulnerability types. We evaluate the effectiveness of our technique in revealing the seeded vulnerabilities and compare against five existing popular analysis tools – Oyente, Securify, Maian, SmartCheck and Mythril. This is the first large scale evaluation of existing tools that compares their effectiveness by running them on a common set of contracts. We find that our technique outperforms all five existing tools in supporting detection of all 8 vulnerability types and in achieving higher precision and recall rate. SolAnalyser was also faster in analysing the different vulnerabilities than any of the existing tools in our experiment. Among existing tools, Securify achieved high precision in detecting integer overflow, underflow, and division by zero but had poor recall rates.","tags":["Blockchain","Smart Contract","Testing","Static Analysis","Assertions","Fault Seeding"],"title":"SolAnalyser: A Framework for Analysing and Testing Smart Contracts","type":"publication"},{"authors":["Chao Peng"],"categories":[],"content":"","date":1559170800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597180930,"objectID":"bb05b16b729dbb9d601d7df746c67617","permalink":"/publication/isstads/","publishdate":"2019-05-30T00:00:00+01:00","relpermalink":"/publication/isstads/","section":"publication","summary":"Testing is an important and challenging part of software development and its effectiveness depends on the quality of test cases. However, there exists no means of measuring quality of tests developed for GPU programs and as a result, no test case generation techniques for GPU programs aiming at high test effectiveness. Existing criteria for sequential and threaded CPU programs cannot be directly applied to GPU programs as GPU follows a completely different memory and execution model.\n\nWe surveyed existing work on GPU program verification and bug fixes of open source GPU programs. Based on our findings, we define barrier, branch and loop coverage criteria and propose a set of mutation operators to measure fault finding capabilities of test cases. CLTestCheck, a framework for measuring quality of tests developed for GPU programs by code coverage analysis, fault seeding and work-group schedule amplification has been developed and evaluated using industry standard benchmarks. Experiments show that the framework is able to automatically measure test effectiveness and reveal unusual behaviours. Our planned work includes data flow coverage adopted for GPU programs to probe the underlying cause of unusual kernel behaviours and a more comprehensive work-group scheduler. We also plan to design and develop an automatic test case generator aiming at generating high quality test suites for GPU programs.","tags":["Software Testing","Code Coverage","Fault Finding","Data Race","GPU","OpenCL","Test Case Generation"],"title":"On the Correctness of GPU Programs","type":"publication"},{"authors":["Chao Peng","Ajitha Rajan"],"categories":[],"content":"","date":1554505200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597180930,"objectID":"7e0be6aad3b6512a7eefe822ae06aaae","permalink":"/publication/cltestcheck/","publishdate":"2019-04-06T00:00:00+01:00","relpermalink":"/publication/cltestcheck/","section":"publication","summary":"Massive parallelism, and energy efficiency of GPUs, along with advances in their programmability with OpenCL and CUDA programming models have made them attractive for general-purpose computations across many application domains. Techniques for testing GPU kernels have emerged recently to aid the construction of correct GPU software. However, there exists no means of measuring quality and effectiveness of tests developed for GPU kernels. Traditional coverage criteria over CPU programs is not adequate over GPU kernels as it uses a completely different programming model and the faults encountered may be specific to the GPU architecture. GPUs have SIMT (single instruction, multiple thread) execution model that executes batches of threads (work groups) in lock-step, i.e all threads in a work group execute the same instruction but on different data.\n\nWe address this need in this paper and present a framework, CLTestCheck, for assessing quality of test suites developed for OpenCL kernels. The framework has the following capabilities, 1. Measures kernel code coverage using three different coverage metrics that are inspired by faults found in real kernel code, 2. Seeds different types of faults in kernel code and measures fault finding capability of test suite, 3. Simulates different work group schedules to check for potential data races with the given test suite. We conducted empirical evaluation of CLTestCheck on a collection of 82 publicly available GPU kernels and test suites. We found that CLTestCheck is capable of automatically measuring effectiveness of test suites, in terms of kernel code coverage, fault finding and revealing data races in real OpenCL kernels.","tags":["Testing","Code Coverage","Fault Finding","Data Race","Mutation Testing","GPU","OpenCL"],"title":"CLTestCheck: Measuring Test Effectiveness for GPU Kernels","type":"publication"}]