
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Chao Peng （彭péng, 超chāo）\nI am a Senior Research Scientist at ByteDance (字节跳动). I received my PhD degree from Laboratory for Foundations of Computer Science (LFCS), The University of Edinburgh under supervision of Prof. Ajitha Rajan.\nAt ByteDance, I lead the Software Engineering Lab, where we conduct research on AI agents for software engineering. My research interest lies in the area of software testing, program repair and optimisations, and the synergy with machine learning and compiler techniques. I am also responsible for academic development and university collaboration.\nOutside of work, I enjoy going to the gym.\nEmail:\nprefix=\u0026#34;chao.peng\u0026#34; domain=\u0026#34;acm\u0026#34; email=${prefix}@${domain}.org Schedule time to talk with me","date":1745794800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1745794800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Chao Peng （彭péng, 超chāo）\nI am a Senior Research Scientist at ByteDance (字节跳动). I received my PhD degree from Laboratory for Foundations of Computer Science (LFCS), The University of Edinburgh under supervision of Prof.","tags":null,"title":"Chao Peng","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Hugo Blox Builder’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://chao-peng.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Hugo Blox Builder's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Xiaoyun Liang","Jingyi Ren","Jiayi Qi","Chao Peng","Bo Jiang"],"categories":[],"content":"","date":1745794800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1745794800,"objectID":"85288787e88e79f9e222127c4ee9edde","permalink":"https://chao-peng.github.io/publication/icse25seip/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/icse25seip/","section":"publication","summary":"Large Language Models (LLMs) have become increasingly integral to enhancing developer productivity, particularly in code generation, comprehension, and repair tasks. However, fine-tuning these models with high-quality, real-world data is challenging due to privacy concerns and the lack of accessible, labeled datasets. In this paper, we present DialogAgent, an automated tool for generating synthetic training data that closely mimics real developer interactions within Integrated Development Environments (IDEs). DialogAgent enables the production of diverse, high-fidelity query-response pairs by simulating multi-turn dialogues and contextual behaviors observed in real-world programming scenarios. The tool significantly reduces the reliance on manual data generation, increasing efficiency by 4.8 times compared to traditional methods. Our experiments and online deployment demonstrate substantial improvements in model performance for code-related question-answering tasks: the acceptance rate of responses generated by our in-house model is improved by 33%, after training on synthesized data generated by DialogAgent.","tags":["LLM","Code Question Answering","Data Production"],"title":"DialogAgent: An Auto-engagement Agent for Code Question Answering Data Production","type":"publication"},{"authors":["Zexiong Ma","Chao Peng","Pengfei Gao","Xiangxin Meng","Yanzhen Zou","Bing Xie"],"categories":[],"content":"","date":1740610800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740610800,"objectID":"19e3733be07f2ca6e91e01b1839efa63","permalink":"https://chao-peng.github.io/publication/arxiv24sorft/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/arxiv24sorft/","section":"publication","summary":"Mainstream issue-resolving frameworks predominantly rely on commercial models, leading to high costs and privacy concerns. Existing training approaches for issue resolving struggle with poor generalization and fail to fully leverage open-source development resources. We propose Subtask-oriented Reinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue resolving capability of LLMs. We decomposes issue resolving into structured subtasks: file localization, function localization, line localization, and code edit generation. SoRFT consists of two training stages: (1) rejection-sampled supervised fine-tuning, Chain of Thought (CoT) data is filtered using ground-truth before fine-tuning the LLM, and (2) rule-based reinforcement learning, which leverages PPO with ground-truth based rewards. We evaluate the SoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving state-of-the-art (SOTA) performance among open-source models (e.g., resolve 21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental results demonstrate that SoRFT significantly enhances issue-resolving performance, improves model generalization, and provides a cost-efficient alternative to commercial models.","tags":["Reinforcement Learning","Large Language Models","Agents","Issue Resolving","Fault Localisation"],"title":"SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning","type":"publication"},{"authors":["Ruida Hu","Chao Peng","Xinchen Wang","Cuiyun Gao"],"categories":[],"content":"","date":1739919600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1739919600,"objectID":"54215b9a8f2a19dd550b5fe336cd451d","permalink":"https://chao-peng.github.io/publication/arxiv24repo2run/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/arxiv24repo2run/","section":"publication","summary":"Environment configuration is a critical yet time-consuming step in software development, especially when dealing with unfamiliar code repositories. While Large Language Models (LLMs) demonstrate the potential to accomplish software engineering tasks, existing methods for environment configuration often rely on manual efforts or fragile scripts, leading to inefficiencies and unreliable outcomes. We introduce Repo2Run, the first LLM-based agent designed to fully automate environment configuration and generate executable Dockerfiles for arbitrary Python repositories. We address two major challenges: (1) enabling the LLM agent to configure environments within isolated Docker containers, and (2) ensuring the successful configuration process is recorded and accurately transferred to a Dockerfile without error. To achieve this, we propose atomic configuration synthesis, featuring a dual-environment architecture (internal and external environment) with a rollback mechanism to prevent environment \"pollution\" from failed commands, guaranteeing atomic execution (execute fully or not at all) and a Dockerfile generator to transfer successful configuration steps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark of 420 recent Python repositories with unit tests, where it achieves an 86.0% success rate, outperforming the best baseline by 63.9%. Repo2Run is available at [this https URL.](https://github.com/bytedance/Repo2Run)","tags":["Bug Reproduction","Large Language Models","Agents"],"title":"An LLM-based Agent for Reliable Docker Environment Configuration","type":"publication"},{"authors":["Bowen Li","Wenhan Wu","Ziwei Tang","Lin Shi","John Yang","Jinyang Li","Shunyu Yao","Chen Qian","Binyuan Hui","Qicheng Zhang","Zhiyin Yu","He Du","Ping Yang","Dahua Lin","Chao Peng","Kai Chen"],"categories":[],"content":"","date":1737414000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737414000,"objectID":"243bb8406f11de99438bfe4c41a49bee","permalink":"https://chao-peng.github.io/publication/coling25/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/coling25/","section":"publication","summary":"Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of coding, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. In this case study, we explore the performance of LLMs across the entire software development lifecycle with DevEval, encompassing stages including software design, environment setup, implementation, acceptance testing, and unit testing. DevEval features four programming languages, multiple domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4, fail to solve the challenges presented within DevEval. Our findings offer actionable insights for the future development of LLMs toward real-world programming applications.","tags":["LLM","Code Generation","Software Engineering","Evaluation","Benchmark"],"title":"Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study","type":"publication"},{"authors":["Jialiang Chen","Kaifa Zhao","Jie Liu","Chao Peng","Jierui Liu","Hang Zhu","Pengfei Gao","Ping Yang","Shuiguang Deng"],"categories":[],"content":"","date":1736204400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1736204400,"objectID":"312d23345f79100b92e46a8a37f9d60a","permalink":"https://chao-peng.github.io/publication/arxiv24coreqa/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/arxiv24coreqa/","section":"publication","summary":"Large language models that enhance software development tasks, such as code generation, code completion, and code question answering (QA), have been extensively studied in both academia and the industry. The models are integrated into popular intelligent IDEs like JetBrains and Cursor. Current benchmarks for evaluating models' code comprehension capabilities primarily focus on code generation or completion, often neglecting QA, which is a crucial aspect of understanding code. Existing code QA benchmarks are derived from code comments with predefined patterns (e.g., CodeQA) or focus on specific domains, such as education (e.g., CS1QA). These benchmarks fail to capture the real-world complexity of software engineering and user requirements for understanding code repositories. To address this gap, we introduce CoReQA, a benchmark for Code Repository-level question answering, constructed from GitHub issues and comments from 176 popular repositories across four programming languages. Since questions and answers may include both natural language and code snippets, traditional evaluation metrics such as BLEU are inadequate for assessing repository-level QA performance. Thus, we provide an LLM-as-a-judge framework to evaluate QA performance from five aspects. Based on CoReQA, we evaluate the performance of three baselines, including two short-context models using generic retrieval strategies and one long-context model that utilizes the entire repository context. Evaluation results show that state-of-the-art proprietary and long-context models struggle to address repository-level questions effectively. Our analysis highlights the limitations of language models in assisting developers in understanding repositories and suggests future directions for improving repository comprehension systems through effective context retrieval methodologies.","tags":["Repository-level dataset","Code Benchmark"],"title":"CoReQA: Uncovering Potentials of Language Models in Code Repository Question Answering","type":"publication"},{"authors":["Foivos Tsimpourlas","Chao Peng","Carlos Rosuero","Ping Yang","Ajitha Rajan"],"categories":[],"content":"","date":1733871600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733871600,"objectID":"c651f10a4c5960285292113a34455382","permalink":"https://chao-peng.github.io/publication/arxiv24gooracle/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/arxiv24gooracle/","section":"publication","summary":"The Go programming language has gained significant traction for developing software, especially in various infrastructure systems. Nonetheless, concurrency bugs have become a prevalent issue within Go, presenting a unique challenge due to the language's dual concurrency mechanisms-communicating sequential processes and shared memory. Detecting concurrency bugs and accurately classifying program executions as pass or fail presents an immense challenge, even for domain experts. We conducted a survey with expert developers at Bytedance that confirmed this challenge. Our work seeks to address the test oracle problem for Go programs, to automatically classify test executions as pass or fail. This problem has not been investigated in the literature for Go programs owing to its distinctive programming model. Our approach involves collecting both passing and failing execution traces from various subject Go programs. We capture a comprehensive array of execution events using the native Go execution tracer. Subsequently, we preprocess and encode these traces before training a transformer-based neural network to effectively classify the traces as either passing or failing. The evaluation of our approach encompasses 8 subject programs sourced from the GoBench repository. These subject programs are routinely used as benchmarks in an industry setting. Encouragingly, our test oracle, Go-Oracle, demonstrates high accuracies even when operating with a limited dataset, showcasing the efficacy and potential of our methodology. Developers at Bytedance strongly agreed that they would use the Go-Oracle tool over the current practice of manual inspections to classify tests for Go programs as pass or fail.","tags":["Test Oracle","Concurrency","Go","Neural Network"],"title":"Go-Oracle: Automated Test Oracle for Go Concurrency Bugs","type":"publication"},{"authors":["Ruida Hu","Chao Peng","Jingyi Ren","Bo Jiang","Xiangxin Meng","Qinyun Wu","Pengfei Gao","Xinchen Wang","Cuiyun Gao"],"categories":[],"content":"","date":1732662000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732662000,"objectID":"adb994075acb51ee740085239a6671ed","permalink":"https://chao-peng.github.io/publication/arxiv24benchmark/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/arxiv24benchmark/","section":"publication","summary":"Automatically resolving software issues is crucial for software development in practice, impacting the software quality and user experience. The process of resolving real-world issues encompasses tasks such as question-answering (QA), fault localization, and code editing. Existing benchmarks such as HumanEval fall short in their ability to assess LLMs' proficiency in solving issues within a codebase. Although benchmarks like SWE-Bench are designed to evaluate the LLMs' capability to handle real-world GitHub issues, the end-to-end evaluation method cannot provide granular insights on the performance of subtasks involved in issue solving. To address existing deficiencies in benchmarking LLMs for practical software engineering tasks, we introduce FAUN-Eval, a benchmark specifically designed to evaluate the Fine-grAined issUe solviNg capabilities of LLMs. FAUN-Eval systematically assesses LLMs across three distinct tasks: QA, fault localization, and code editing. This benchmark is constructed using a dataset curated from 30 well-known GitHub repositories. For each entry, issue and pull request (PR) pairs are meticulously compiled and validated using cross-referencing and keyword verification methods. FAUN-Eval includes 300 entries and employs both LLM and manual checks to ensure data quality. We evaluate ten LLMs with FAUN-Eval, including four closed-source and six open-source models. Our experimental results reveal several key findings. We find that the top-performing LLMs differ across the different tasks. Additionally, features in issues may lead LLMs to generate incorrect information. Moreover, models may vary in their proficiency with texts of different lengths.","tags":["Code Question Answering","Large Language Models","Mining Software Repository"],"title":"A Real-World Benchmark for Evaluating Fine-Grained Issue Solving Capabilities of Large Language Models","type":"publication"},{"authors":["Xinchen Wang","Pengfei Gao","Xiangxin Meng","Chao Peng","Ruida Hu","Yun Lin","Cuiyun Gao"],"categories":[],"content":"","date":1732662000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732662000,"objectID":"d3781c79145c82cdca359084971e8ad7","permalink":"https://chao-peng.github.io/publication/arxiv24aegis/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/arxiv24aegis/","section":"publication","summary":"In software maintenance, bug reproduction is essential for effective fault localization and repair. Manually writing reproduction scripts is a time-consuming task with high requirements for developers. Hence, automation of bug reproduction has increasingly attracted attention from researchers and practitioners. However, the existing studies on bug reproduction are generally limited to specific bug types such as program crashes, and hard to be applied to general bug reproduction. In this paper, considering the superior performance of agent-based methods in code intelligence tasks, we focus on designing an agent-based framework for the task. Directly employing agents would lead to limited bug reproduction performance, due to entangled subtasks, lengthy retrieved context, and unregulated actions. To mitigate the challenges, we propose an Automated gEneral buG reproductIon Scripts generation framework, named AEGIS, which is the first agent-based framework for the task. AEGIS mainly contains two modules: (1) A concise context construction module, which aims to guide the code agent in extracting structured information from issue descriptions, identifying issue-related code with detailed explanations, and integrating these elements to construct the concise context; (2) A FSM-based multi-feedback optimization module to further regulate the behavior of the code agent within the finite state machine (FSM), ensuring a controlled and efficient script generation process based on multi-dimensional feedback. Extensive experiments on the public benchmark dataset show that AEGIS outperforms the state-of-the-art baseline by 23.0% in F-\u003eP metric. In addition, the bug reproduction scripts generated by AEGIS can improve the relative resolved rate of Agentless by 12.5%.","tags":["Bug Reproduction","Large Language Models","Agents"],"title":"AEGIS: An Agent-based Framework for General Bug Reproduction from Issue Descriptions","type":"publication"},{"authors":["Zhanming Guan","Junlin Liu","Jierui Liu","Chao Peng","Dexin Liu","Ningyuan Sun","Bo Jiang","Wenchao Li","Jie Liu","Hang Zhu"],"categories":[],"content":"","date":1732662000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732662000,"objectID":"0f55a6563d97aac1c8f8d5e4029b4c75","permalink":"https://chao-peng.github.io/publication/arxiv24contextmodule/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/arxiv24contextmodule/","section":"publication","summary":"Large Language Models (LLMs) have demonstrated impressive capabilities in code completion tasks, where they assist developers by predicting and generating new code in real-time. However, existing LLM-based code completion systems primarily rely on the immediate context of the file being edited, often missing valuable repository-level information, user behaviour and edit history that could improve suggestion accuracy. Additionally, challenges such as efficiently retrieving relevant code snippets from large repositories, incorporating user behavior, and balancing accuracy with low-latency requirements in production environments remain unresolved. In this paper, we propose ContextModule, a framework designed to enhance LLM-based code completion by retrieving and integrating three types of contextual information from the repository: user behavior-based code, similar code snippets, and critical symbol definitions. By capturing user interactions across files and leveraging repository-wide static analysis, ContextModule improves the relevance and precision of generated code. We implement performance optimizations, such as index caching, to ensure the system meets the latency constraints of real-world coding environments. Experimental results and industrial practise demonstrate that ContextModule significantly improves code completion accuracy and user acceptance rates.","tags":["Code Completion","Large Language Models","Repository Context","Code Knowledge Graph"],"title":"ContextModule: Improving Code Completion via Repository-level Contextual Information","type":"publication"},{"authors":["Xiangxin Meng","Zexiong Ma","Pengfei Gao","Chao Peng"],"categories":[],"content":"","date":1731625200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1731625200,"objectID":"74b395d51d34e0130f5cb6588b577bf7","permalink":"https://chao-peng.github.io/publication/arxiv24empirical/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/arxiv24empirical/","section":"publication","summary":"Large language models (LLMs) and LLM-based Agents have been applied to fix bugs automatically, demonstrating the capability in addressing software defects by engaging in development environment interaction, iterative validation and code modification. However, systematic analysis of these agent and non-agent systems remain limited, particularly regarding performance variations among top-performing ones. In this paper, we examine seven proprietary and open-source systems on the SWE-bench Lite benchmark for automated bug fixing. We first assess each system's overall performance, noting instances solvable by all or none of these sytems, and explore why some instances are uniquely solved by specific system types. We also compare fault localization accuracy at file and line levels and evaluate bug reproduction capabilities, identifying instances solvable only through dynamic reproduction. Through analysis, we concluded that further optimization is needed in both the LLM itself and the design of Agentic flow to improve the effectiveness of the Agent in bug fixing.","tags":["Bug Fixing","Large Language Models","Agents","Empirical Study"],"title":"An Empirical Study on LLM-based Agents for Automated Bug Fixing","type":"publication"},{"authors":["Chao Peng","Qinyun Wu","Jiangchao Liu","Jierui Liu","Bo Jiang","Mengqian Xu","Yinghao Wang","Xia Liu","Ping Yang"],"categories":[],"content":"","date":1729983600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1729983600,"objectID":"cf04a6c97b3a6ea81a4f40f459ce4080","permalink":"https://chao-peng.github.io/publication/ase24nier/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/ase24nier/","section":"publication","summary":"Large language models (LLMs) have revolutionized code completion tasks. IDE plugins such as Copilot can generate code recommendations, saving developers significant time and effort. However, current evaluation methods for code completion are limited by their reliance on static code benchmarks, which do not consider human interactions and evolving repositories. This paper proposes RepoSim, a novel benchmark designed to evaluate code completion tasks by simulating the evolving process of repositories and incorporating user behaviors. RepoSim leverages data from an IDE plugin, by recording and replaying user behaviors to provide a realistic programming context for evaluation. This allows for the assessment of more complex prompt strategies, such as utilizing recently visited files and incorporating user editing history. Additionally, RepoSim proposes a new metric based on users' acceptance or rejection of predictions, offering a user-centric evaluation criterion. Our preliminary evaluation demonstrates that incorporating users' recent edit history into prompts significantly improves the quality of LLM-generated code, highlighting the importance of temporal context in code completion. RepoSim represents a significant advancement in benchmarking tools, offering a realistic and user-focused framework for evaluating code completion performance.","tags":["Software Testing","Code Completion","LLM4Code","AI4SE"],"title":"RepoSim: Evaluating Prompt Strategies for Code Completion via User Behavior Simulation","type":"publication"},{"authors":["Yizhou Liu","Pengfei Gao","Xinchen Wang","Jie Liu","Yexuan Shi","Zhao Zhang","Chao Peng"],"categories":[],"content":"","date":1725404400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725404400,"objectID":"537e4da14dd971585a734846f325837f","permalink":"https://chao-peng.github.io/publication/arxiv24marscodeagent/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/arxiv24marscodeagent/","section":"publication","summary":"Recent advances in large language models (LLMs) have shown significant potential to automate various software development tasks, including code completion, test generation, and bug fixing. However, the application of LLMs for automated bug fixing remains challenging due to the complexity and diversity of real-world software systems. In this paper, we introduce MarsCode Agent, a novel framework that leverages LLMs to automatically identify and repair bugs in software code. MarsCode Agent combines the power of LLMs with advanced code analysis techniques to accurately localize faults and generate patches. Our approach follows a systematic process of planning, bug reproduction, fault localization, candidate patch generation, and validation to ensure high-quality bug fixes. We evaluated MarsCode Agent on SWE-bench, a comprehensive benchmark of real-world software projects, and our results show that MarsCode Agent achieves a high success rate in bug fixing compared to most of the existing automated approaches.","tags":["Bug Fixing","Large Language Models","Automated Program Repair","Agents"],"title":"MarsCode Agent: AI-native Automated Bug Fixing","type":"publication"},{"authors":["Qinyun Wu","Chao Peng","Pengfei Gao","Ruida Hu","Haoyu Gan","Bo Jiang","Jinhe Tang","Zhiwen Deng","Zhanming Guan","Cuiyun Gao","Xia Liu","Ping Yang"],"categories":[],"content":"","date":1722985200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722985200,"objectID":"3459413ae96641bc2eea4980726def8a","permalink":"https://chao-peng.github.io/publication/arxiv24repomastereval/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/arxiv24repomastereval/","section":"publication","summary":"With the growing reliance on automated code completion tools in software development, the need for robust evaluation benchmarks has become critical. However, existing benchmarks focus more on code generation tasks in function and class level and provide rich text description to prompt the model. By contrast, such descriptive prompt is commonly unavailable in real development and code completion can occur in wider range of situations such as in the middle of a function or a code block. These limitations makes the evaluation poorly align with the practical scenarios of code completion tools. In this paper, we propose RepoMasterEval, a novel benchmark for evaluating code completion models constructed from real-world Python and TypeScript repositories. Each benchmark datum is generated by masking a code snippet (ground truth) from one source code file with existing test suites. To improve test accuracy of model generated code, we employ mutation testing to measure the effectiveness of the test cases and we manually crafted new test cases for those test suites with low mutation score. Our empirical evaluation on 6 state-of-the-art models shows that test argumentation is critical in improving the accuracy of the benchmark and RepoMasterEval is able to report difference in model performance in real-world scenarios. The deployment of RepoMasterEval in a collaborated company for one month also revealed that the benchmark is useful to give accurate feedback during model training and the score is in high correlation with the model's performance in practice. Based on our findings, we call for the software engineering community to build more LLM benchmarks tailored for code generation tools taking the practical and complex development environment into consideration.","tags":["Code Generation","Benchmark","Large Language Model"],"title":"RepoMasterEval: Evaluating Code Completion via Real-World Repositories","type":"publication"},{"authors":["Zhu Tao","Yongqiang Gao","Jiayi Qi","Chao Peng","Qinyun Wu","Xiang Chen","Ping Yang"],"categories":[],"content":"","date":1720998000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720998000,"objectID":"4fc29604fbb5b05d3e3eea880ad1a4fb","permalink":"https://chao-peng.github.io/publication/fse24/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/fse24/","section":"publication","summary":"A wide variety of device models, screen resolutions and operating systems have emerged with recent advances in mobile devices. As a result, the graphical user interface (GUI) layout in mobile apps has become increasingly complex due to this market fragmentation, with rapid iterations being the norm. Testing page layout issues under these circumstances hence becomes a resource-intensive task, requiring significant manpower and effort due to the vast number of device models and screen resolution adaptations. One of the most challenging issues to cover manually is multi-model and cross-version layout verification for the same GUI page. To address this issue, we propose Neat, a non-intrusive end-to-end mobile app layout similarity measurement tool that utilizes computer vision techniques for GUI element detection, layout feature extraction, and similarity metrics. Our empirical evaluation and industrial application have demonstrated that our approach is effective in improving the efficiency of layout assertion testing and ensuring application quality.","tags":["Software Testing","Deep Learning","AI4SE"],"title":"Neat: Mobile App Layout Similarity Comparison based on Graph Convolutional Networks","type":"publication"},{"authors":["Chao Peng","Zhengwei Lv","Jiarong Fu","Jiayuan Liang","Zhao Zhang","Ajitha Rajan","Ping Yang"],"categories":[],"content":"","date":1712876400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712876400,"objectID":"d341d6081bcb23c6d92299a6a8de69d8","permalink":"https://chao-peng.github.io/publication/icse24seip/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/icse24seip/","section":"publication","summary":"Android Apps are frequently updated to keep up with changing user, hardware, and business demands. Ensuring the correctness of App updates through extensive testing is crucial to avoid potential bugs reaching the end user. Existing Android testing tools generate GUI events that focus on improving the test coverage of the entire App rather than prioritising updates and its impacted elements. Recent research has proposed change-focused testing but relies on random exploration to exercise the updates and impacted GUI elements that is ineffective and slow for large complex Apps with a huge input exploration space. At ByteDance, our established model-based GUI testing tool, Fastbot2, has been in successful deployment for nearly three years. Fastbot2 leverages event-activity transition models derived from past App explorations to achieve enhanced test coverage efficiently. A pivotal insight we gained is that the knowledge of event-activity transitions is equally valuable in effectively targeting changes introduced by App updates. This insight propelled our proposal for directed testing of App updates with Hawkeye. Hawkeye excels in prioritizing GUI actions associated with code changes through deep reinforcement learning from historical exploration data.\nIn our empirical evaluation, we rigorously compared Hawkeye with state-of-the-art tools like Fastbot2 and ARES. We utilized 10 popular open-source Apps and a notable commercial App for this evaluation. The results showcased that Hawkeye consistently outperforms Fastbot2 and ARES in generating GUI event sequences that effectively target changed functions, both in open-source and commercial App contexts.\nIn real-world industrial deployment, Hawkeye is seamlessly integrated into our development pipeline, performing smoke testing for merge requests in a complex commercial App. The positive feedback received from our App development teams further affirmed Hawkeye's ability in testing App updates effectively.","tags":["Software Testing","AI2SE"],"title":"Hawkeye: Change-targeted Testing for Android Apps based on Deep Reinforcement Learning","type":"publication"},{"authors":["Xiaoyun Liang","Jiayi Qi","Yongqiang Gao","Chao Peng","Ping Yang"],"categories":null,"content":"","date":1701558000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701558000,"objectID":"7e00d63eaae1d8bd72380ad0d29e4b07","permalink":"https://chao-peng.github.io/publication/fse23/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/fse23/","section":"publication","summary":"With the advancement of device software and hardware performance, and the evolution of game engines, an increasing number of emerging high-quality games are captivating game players from all around the world who speak different languages. However, due to the vast fragmentation of the device and platform market, a well-tested game may still experience text glitches when installed on a new device with an unseen screen resolution and system version, which can significantly impact the user experience. In our testing pipeline, current testing techniques for identifying multilingual text glitches are laborious and inefficient. In this paper, we present AG3, which offers intelligent game traversal, precise visual text glitch detection, and integrated quality report generation capabilities. Our empirical evaluation and internal industrial deployment demonstrate that AG3 can detect various real-world multilingual text glitches with minimal human involvement.","tags":["Software Testing","AI2SE"],"title":"AG3: Automated Game GUI Text Glitch Detection based on Computer Vision","type":"publication"},{"authors":null,"categories":null,"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic! Organize your notebooks Place the notebooks that you would like to publish in a notebooks folder at the root of your website.\nImport the notebooks into your site pipx install academic academic import \u0026#39;notebooks/**.ipynb\u0026#39; content/post/ --verbose The notebooks will be published to the folder you specify above. In this case, they will be published to your content/post/ folder.\n","date":1699056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699056000,"objectID":"94fa5e486d3bf3e0941e2ff6e7126c06","permalink":"https://chao-peng.github.io/post/blog-with-jupyter/","publishdate":"2023-11-04T00:00:00Z","relpermalink":"/post/blog-with-jupyter/","section":"post","summary":"Easily blog from Jupyter notebooks!","tags":null,"title":"Blog with Jupyter Notebooks!","type":"post"},{"authors":["Zongze Jiang","Ming Wen","Yixin Yang","Chao Peng","Ping Yang","Hai Jin"],"categories":[],"content":"","date":1694386800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694386800,"objectID":"9de41008d0f4f0536bddfee2ea4a4fb8","permalink":"https://chao-peng.github.io/publication/ase23/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/ase23/","section":"publication","summary":"The Go language (Go/Golang) has been attracting increasing attention from the industry in recent years due to its straightforward syntax, strong concurrency support, and ease of deployment. This programming language encourages developers to use channel-based concurrency, which simplifies development. Unfortunately, it also introduces new concurrency problems that differ from those caused by the mechanism of shared memory concurrency. Despite this, there are only few works that aim to detect these Go-specific concurrency issues. Even state-of-the-art testing tools will miss critical concurrent bugs that require fine-grained and effective interleaving exploration.\n\nThis paper presents GoPie, a novel testing approach for detecting Go concurrent bugs through primitive-constrained interleaving exploration. GoPie utilizes execution histories to identify new interleavings instead of relying on exhaustive exploration or random scheduling. To evaluate its effectiveness, we applied GoPie on existing benchmarks and large-scale open-source projects. Results show that GoPie can effectively explore concurrent interleavings and detect significantly more bugs in the benchmark. Furthermore, it uncovered 11 unique previously unknown concurrent bugs, and 9 of which have been confirmed.","tags":["Software Testing"],"title":"Effective Concurrency Testing for Go via Directional Primitive-constrained Interleaving Exploration","type":"publication"},{"authors":["Siwei Wang","Xue Mao","Ziguang Gao","Yujun Gao","Qucheng Shen","Chao Peng"],"categories":[],"content":"","date":1686697200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686697200,"objectID":"7d76f3fb017139eed5878d3fdbdf8213","permalink":"https://chao-peng.github.io/publication/ease23/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/ease23/","section":"publication","summary":"Automated test generation has been extensively studied for dynamically compiled or typed programming languages like Java and Python. However, Go, a popular statically compiled and typed programming language for server application development, has received limited support from existing tools. To address this gap, we present NxtUnit, an automatic unit test generation tool for Go that uses random testing and is well-suited for microservice architecture. NxtUnit employs a random approach to generate unit tests quickly, making it ideal for smoke testing and providing quick quality feedback. It comes with three types of interfaces: an integrated development environment (IDE) plugin, a command-line interface (CLI), and a browser-based platform. The plugin and CLI tool allow engineers to write unit tests more efficiently, while the platform provides unit test visualization and asynchronous unit test generation. We evaluated NxtUnit by generating unit tests for 13 open-source repositories and 500 ByteDance in-house repositories, resulting in a code coverage of 20.74% for in-house repositories. We conducted a survey among ByteDance engineers and found that NxtUnit can save them 48% of the time on writing unit tests. We have made the CLI tool available at https://github.com/bytedance/nxt_unit.","tags":["Software Testing"],"title":"NxtUnit: Automated Unit Test Generation for Go","type":"publication"},{"authors":["Jingling Sun","Ting Su","Kai Liu","Chao Peng","Zhao Zhang","Geguang Pu","Tao Xie","Zhendong Su"],"categories":[],"content":"","date":1671404400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671404400,"objectID":"2c23842d4884856231c85a76f6f09104","permalink":"https://chao-peng.github.io/publication/tse22/","publishdate":"2019-09-30T00:00:00+01:00","relpermalink":"/publication/tse22/","section":"publication","summary":"Android, the most popular mobile system, offers a number of user-configurable system settings (e.g., network, location, and permission) for controlling devices and apps. Even popular, well-tested apps may fail to properly adapt their behaviors to diverse setting changes, thus frustrating their users. However, there exists no effort to systematically investigate such defects. To this end, we conduct the first large-scale empirical study to understand and characterize these system setting-related defects(in short as “setting defects”), which reside in apps and are triggered by system setting changes. We devote substantial manual effort (over four person-months) to analyze 1,074 setting defects from 180 popular apps on GitHub. We investigate the impact, root causes, and consequences of these setting defects and their correlations. We find that (1) setting defects have a wide impact on apps’ correctness with diverse root causes, (2) the majority of these defects (≈70.7%) cause non-crashing (logic) failures, and (3) some correlations exist between the setting categories, root causes, and consequences. Motivated and informed by these findings, we propose two bug-finding techniques that can synergistically detect setting defects from both the GUI and code levels. Specifically, at the GUI level,we design and introduce setting-wise metamorphic fuzzing, the first automated dynamic testing technique to detect setting defects(causing crash and non-crashing failures, respectively) for Android apps. We implement this technique as an end-to-end, automatedGUI testing tool named SETDROID. At the code level, we distill two major fault patterns and implement a static analysis tool named SETCHECKER to identify potential setting defects. We evaluate SETDROID and SETCHECKER on 26 popular, open-source Android apps, and they find 48 unique, previously-unknown setting defects. To date, 35 have been confirmed and 21 have been fixed by app developers. We also apply SETDROID and SETCHECKER on five highly popular industrial apps, namely WeChat, QQMail, TikTok,CapCut, and Alipay HK, all of which each have billions of monthly active users. SETDROID successfully detects 17 previously unknown setting defects in these apps’ latest releases, and all defects have been confirmed and fixed by the app vendors. After that, we collaborate with ByteDance and deploy these two bug-finding techniques internally to stress-test TikTok, one of its major app products.Within a two-month testing campaign, SETDROID successfully finds 53 setting defects, and SETCHECKER finds 22 ones. So far, 59have been confirmed and 31 have been fixed. All these defects escaped from prior developer testing. By now, SETDROIDhas been integrated into ByteDance’s official app testing infrastructure named FASTBOT for daily testing. These results demonstrate the strong effectiveness and practicality of our proposed techniques.","tags":["Empirical study","System Settings","Android Apps","GUI Testing","Static Analysis"],"title":"Characterizing and Finding System Setting-Related Defects in Android Apps","type":"publication"},{"authors":["Zhengwei Lv","Chao Peng","Zhao Zhang","Ting Su","Kai Liu","Ping Yang"],"categories":[],"content":"","date":1665356400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665356400,"objectID":"c2701c8f26b2b0c09607e58d56a75cd8","permalink":"https://chao-peng.github.io/publication/ase22/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/ase22/","section":"publication","summary":"In the industrial setting, mobile apps undergo frequent updates to catch up with the changing real-world requirements. It leads to the strong practical demands of continuous testing, i.e., obtaining quick feedback on app quality during development. However, existing automated GUI testing techniques fall short in this scenario as they simply run an app version from scratch and do not reuse the knowledge from previous testing runs to accelerate the testing cycle. To fill this important gap, we introduce a reusable automated model-based GUI testing technique. Our key insight is that the knowledge of event-activity transitions from the previous testing runs, i.e., executing which events can reach which activities, is valuable for guiding the follow-up testing runs to quickly cover major app functionalities. To this end, we propose (1) a probabilistic model to memorize and leverage this knowledge during testing, and (2) design a model-based guided testing strategy (enhanced by a reinforcement learning algorithm), to achieve faster-and-higher coverage testing. We implemented our technique as an automated testing tool named Fastbot2. Our evaluation on the two popular industrial apps (with billions of user installations) from ByteDance, Douyin and Toutiao, shows that Fastbot2 outperforms the state-of-the-art testing tools (Monkey, APE and Stoat) in both activity coverage and fault detection in the context of continuous testing. To date, Fastbot2 has been deployed in the CI pipeline at ByteDance for nearly two years, and 50.8% of the developer-fixed crash bugs were reported by Fastbot2, which significantly improves app quality. Fastbot2 has been made publicly available to benefit the community at: https://github.com/bytedance/Fastbot_Android. To date, it has received 500+ stars on GitHub and been used by many app vendors and individual developers to test their apps.","tags":["Software Testing","AI2SE"],"title":"Fastbot2: Reusable Automated Model-based GUI Testing for Android Enhanced by Reinforcement Learning","type":"publication"},{"authors":["Chao Peng","Yujun Gao","Ping Yang"],"categories":[],"content":"","date":1664146800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664146800,"objectID":"e044b1a35b2e726889c4ead48a9346a5","permalink":"https://chao-peng.github.io/publication/icsme22a/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/icsme22a/","section":"publication","summary":"A server API bug could have a huge impact on the operation of other servers and clients relying on that API, resulting in service downtime and financial losses. A common practice of server API testing inside enterprises is writing test inputs and assertions manually, and the test effectiveness depends largely on testers' carefulness, expertise and domain knowledge. Writing test cases for complicated business scenarios with multiple and ordered API calls is also a heavy task that requires a lot of human effort. In this paper, we present the design and deployment of SIT, a fully automated server reliability testing platform at ByteDance that provides capabilities including (1) traffic data generation based on combinatorial testing and fuzzing, (2) scenario testing for complicated business logics and (3) automated test execution with fault localisation in a controlled environment that does not affect online services. SIT has been integrated into the source control system and is triggered when new code change is submitted or configured as scheduled tasks. During the year of 2021, SIT blocked 434 valid issues before they were introduced into the production system.","tags":["Server Testing","Traffic Record and Replay","automated testing"],"title":"Automated Server Testing: an Industrial Experience Report","type":"publication"},{"authors":["Chao Peng","Zhao Zhang","Zhengwei Lv","Ping Yang"],"categories":[],"content":"","date":1664146800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664146800,"objectID":"2efe2e3927cb05a23e55ef4fcb9f3483","permalink":"https://chao-peng.github.io/publication/icsme22b/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/icsme22b/","section":"publication","summary":"Automated GUI testing has been playing a key role to uncover crashes to ensure the stability and robustness of Android apps. Recent research has proposed random, search-based and model-based testing techniques for GUI event generation. In industrial practices, different companies have developed various GUI exploration tools such as Facebook Sapienz, WeChat WeTest and ByteDance Fastbot to test their products. However, these tools are bound to their predefined GUI exploration strategies and lack of the ability to generate human-like actions to test meaningful scenarios. To address these challenges, Humanoid is the first Android testing tool that utilises deep learning to imitate human behaviours and achieves promising results over current model-based methods. However, we find some challenges when applying Humanoid to test our sophisticated commercial apps such as infinite loops and low test coverage. To this end, we performed the first case study on the performance of deep learning techniques using commercial apps to understand the underlying reason of the current weakness of this promising method. Based on our findings, we propose MUBot (Multi-modal User Bot) for human-like Android testing. Our empirical evaluation reveals that MUBot has better performance over Humanoid and Fastbot, our in-house testing tool on coverage achieved and bug-fixing rate on commercial apps.","tags":["Android Testing","Graphical User Interface","Deep Learning","AI2SE"],"title":"MUBot: Learning to Test Large-Scale Commercial Android Apps like a Human","type":"publication"},{"authors":["Sefa Akca","Chao Peng","Ajitha Rajan"],"categories":[],"content":"","date":1633906800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633906800,"objectID":"33bf610efa2b83ae8f95d6d9ede0c9b5","permalink":"https://chao-peng.github.io/publication/esem21/","publishdate":"2021-06-30T00:00:00+01:00","relpermalink":"/publication/esem21/","section":"publication","summary":"Executing, verifying and enforcing credible transactions on permissionless blockchains is done using smart contracts. A key challenge with smart contracts is ensuring their correctness and security. Several test input generation techniques for detecting vulnerabilities in smart contracts have been proposed in the last few years. However,a comparison of proposed techniques to gauge their effectivenessis missing. This paper conducts an empirical evaluation of testing techniques for smart contracts. The testing techniques we evaluated are: (1) Blackbox fuzzing, (2) Adaptive fuzzing, (3) Coverage-guided fuzzing with an SMT solver and (4) Genetic algorithm. We do not consider static analysis tools, as several recent studies have assessed and compared effectiveness of these tools. We evaluate effectiveness of the test generation techniques using (1) Coverage achieved - we use four code coverage metrics targeting smart contracts, (2) Fault finding ability - using aritificially seeded and real security vulnerabilities of different types. We used two datasets in our evaluation - one with 1665 real smart contracts from Etherscan, and another with 90 real contracts with known vulnerabilities to assess fault finding ability. We find Adaptive fuzzing performs best in terms of coverage and fault finding over contractsin both datasets.","tags":["Ethereum","Smart Contract","Fault Seeding","Test Generation","Genetic Algorithm","Fuzzing","SMT Solving","Software Testing"],"title":"Testing Smart Contracts: Which Technique Performs Best?","type":"publication"},{"authors":["Chao Peng","Ajitha Rajan","Tianqin Cai"],"categories":[],"content":"","date":1632697200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632697200,"objectID":"ab826f90d697ac60eb6a49bb4ee2d677","permalink":"https://chao-peng.github.io/publication/icsme21/","publishdate":"2021-06-15T00:00:00+01:00","relpermalink":"/publication/icsme21/","section":"publication","summary":"Android Apps are frequently updated, every couple of weeks, to keep up with changing user, hardware and business demands. Correctness of App updates is checked through extensive testing. Recent research has proposed tools for automated GUI event generation in Android Apps. These techniques, however, are not efficient at checking App updates as the generated GUI events do not prioritise updates, and instead explore other App behaviours. We address this need in this paper with CAT (Change-focused Android GUI Testing). For App updates, at the source code or GUI level, CAT performs change impact analysis to identify GUI elements affected by the update. CAT then generates GUI event sequences to interact with the affected GUI elements. Our empirical evaluations using 21 publicly available open source and 2 commercial Android Apps demonstrate that CAT is able to automatically identify GUI elements affected by App updates, generate and execute GUI event sequences focusing on change-affected GUI elements. Comparison with two popular GUI event generation tools, DroidBot and DroidMate, revealed that CAT was more effective at interacting with the change-affected GUI elements. Finally, CAT was able to detect previously unknown change-related bugs in two open source Apps. Developers of the commercial Apps found CAT was more effective than their in-house GUI testing tool in interacting with changed elements and faster at detecting seeded bugs.","tags":["Software Testing","Android","Graphical User Interface","Program Analysis"],"title":"CAT: Change-focused Android GUI Testing","type":"publication"},{"authors":["Chao Peng","吳恩達"],"categories":["Demo","教程"],"content":"import libr print(\u0026#39;hello\u0026#39;) Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy’s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://chao-peng.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Hugo Blox Builder, the website builder for Hugo","type":"post"},{"authors":["Chao Peng","Ajitha Rajan"],"categories":[],"content":"","date":1582412400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582412400,"objectID":"b4e744db1c5ac7e866207aa9c8c8f18d","permalink":"https://chao-peng.github.io/publication/gpgpu20/","publishdate":"2020-02-23T00:00:00+01:00","relpermalink":"/publication/gpgpu20/","section":"publication","summary":"Graphics Processing Units (GPUs) are massively parallel processors offering performance acceleration and energy efficiency unmatched by current processors (CPUs) in computers. These advantages along with recent advances in the programmability of GPUs have made them attractive for general-purpose computations. Despite the advances in programmability, GPU kernels are hard to code and analyse due to the high complexity of memory sharing patterns, striding patterns for memory accesses, implicit synchronisation, and combinatorial explosion of thread interleavings. Existing few techniques for testing GPU kernels use symbolic execution for test generation that incur a high overhead, have limited scalability and do not handle all data types.\n\nWe propose a test generation technique for OpenCL kernels that combines mutation-based fuzzing and selective constraint solving with the goal of being fast, effective and scalable. Fuzz testing for GPU kernels has not been explored previously. Our approach for fuzz testing randomly mutates input kernel argument values with the goal of increasing branch coverage. When fuzz testing is unable to increase branch coverage with random mutations, we gather path constraints for uncovered branch conditions and invoke the Z3 constraint solver to generate tests for them.\n\nIn addition to the test generator, we also present a schedule amplifier that simulates multiple work-group schedules, with which to execute each of the generated tests. The schedule amplifier is designed to help uncover inter work-group data races. We evaluate the effectiveness of the generated tests and schedule amplifier using 217 kernels from open source projects and industry standard benchmark suites measuring branch coverage and fault finding. We find our test generation technique achieves close to 100% coverage and mutation score for majority of the kernels. Overhead incurred in test generation is small (average of 0.8 seconds). We also confirmed our technique scales easily to large kernels, and can support all OpenCL data types, including complex data structures.","tags":["Software Testing","GPU","OpenCL","Test Case Generation","Fuzz Testing","Constraint Solving","Data Race"],"title":"Automated Test Generation for OpenCL Kernels Using Fuzzing and Constraint Solving","type":"publication"},{"authors":["Chao Peng","Sefa Akca","Ajitha Rajan"],"categories":[],"content":"","date":1569798000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569798000,"objectID":"ba4ae6b6293060997595c6857584542b","permalink":"https://chao-peng.github.io/publication/apsec19a/","publishdate":"2019-09-30T00:00:00+01:00","relpermalink":"/publication/apsec19a/","section":"publication","summary":"Solidity is an object-oriented and high-level language for writing smart contracts that are used to execute, verify and enforce credible transactions on permissionless blockchains. In the last few years, analysis of smart contracts has raised considerable interest and numerous techniques have been proposed to check the presence of vulnerabilities in them. Current techniques lack traceability in source code and have widely differing work flows. There is no single unifying framework for analysis, instrumentation, optimisation and code generation of Solidity contracts.\n\nIn this paper, we present SIF, a comprehensive framework for Solidity contract analysis, query, instrumentation, and code generation. SIF provides support for Solidity contract developers and testers to build source level techniques for analysis, understanding, diagnostics, optimisations and code generation. We show feasibility and applicability of the framework by building practical tools on top of it and running them on 1838 real smart contracts deployed on the Ethereum network.","tags":["High Level Languages","Software Testing","Code Instrumentation","Program Analysis"],"title":"SIF: A Framework for Solidity Contract Instrumentation and Analysis","type":"publication"},{"authors":["Sefa Akca","Ajitha Rajan","Chao Peng"],"categories":[],"content":"","date":1569798000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569798000,"objectID":"47a5592c22582baa3e276481b7a01a72","permalink":"https://chao-peng.github.io/publication/apsec19b/","publishdate":"2019-09-30T00:00:00+01:00","relpermalink":"/publication/apsec19b/","section":"publication","summary":"Executing, verifying and enforcing credible transactions on permissionless blockchains is done using smart contracts. Smart contracts define and execute crucial agreements, and attacks exploiting their vulnerabilities can lead to huge losses, like the failure of the DAO smart contract that resulted in 50 million US Dollars worth of losses. A key challenge with smart contracts is ensuring their correctness and security.\n\nTo address this challenge, we present a fully automated technique, SolAnalyser, for vulnerability detection over Solidity smart contracts that uses both static and dynamic analysis. Analysis techniques for smart contracts in the literature rely on static analysis with a high rate of false positives or lack support for vulnerabilities like out of gas, unchecked send, timestamp dependency. We instrument the original smart contract with property checks and use dynamic analysis to tackle this problem. Our tool, SolAnalyser, supports automated detection of 8 different vulnerability types that currently lack wide support in existing tools, and can easily be extended to support other types. We also implemented a fault seeding tool that injects different types of vulnerabilities in smart contracts. We use the mutated contracts for assessing the effectiveness of different analysis tools.\n\nOur experiment uses 1838 real contracts from which we generate 12866 mutated contracts by artificially seeding 8 different vulnerability types. We evaluate the effectiveness of our technique in revealing the seeded vulnerabilities and compare against five existing popular analysis tools – Oyente, Securify, Maian, SmartCheck and Mythril. This is the first large scale evaluation of existing tools that compares their effectiveness by running them on a common set of contracts. We find that our technique outperforms all five existing tools in supporting detection of all 8 vulnerability types and in achieving higher precision and recall rate. SolAnalyser was also faster in analysing the different vulnerabilities than any of the existing tools in our experiment. Among existing tools, Securify achieved high precision in detecting integer overflow, underflow, and division by zero but had poor recall rates.","tags":["Blockchain","Smart Contract","Testing","Static Analysis","Assertions","Fault Seeding"],"title":"SolAnalyser: A Framework for Analysing and Testing Smart Contracts","type":"publication"},{"authors":null,"categories":null,"content":"Hugo Blox Builder is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you’ll find some examples of the types of technical content that can be rendered with Wowchemy.\nExamples Code Wowchemy supports a Markdown extension for highlighting code syntax. You can customize the styles under the syntax_highlighter option in your config/_default/params.yaml file.\n```python import pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Mindmaps Wowchemy supports a Markdown extension for mindmaps.\nSimply insert a Markdown markmap code block and optionally set the height of the mindmap as shown in the example below.\nA simple mindmap defined as a Markdown list:\n```markmap {height=\u0026#34;200px\u0026#34;} - Hugo Modules - wowchemy - blox-plugins-netlify - blox-plugins-netlify-cms - blox-plugins-reveal ``` renders as\n- Hugo Modules - wowchemy - blox-plugins-netlify - blox-plugins-netlify-cms - blox-plugins-reveal A more advanced mindmap with formatting, code blocks, and math:\n```markmap - Mindmaps - Links - [Wowchemy Docs](https://docs.hugoblox.com/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/HugoBlox/hugo-blox-builder) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ ``` renders as\n- Mindmaps - Links - [Wowchemy Docs](https://docs.hugoblox.com/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/HugoBlox/hugo-blox-builder) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ Charts Wowchemy supports the popular Plotly format for interactive charts.\nSave your Plotly JSON in your page folder, for example line-chart.json, and then add the {{\u0026lt; chart data=\u0026#34;line-chart\u0026#34; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\nYou might also find the Plotly JSON Editor useful.\nMath Wowchemy supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.yaml file.\nTo render inline or block math, wrap your LaTeX math with {{\u0026lt; math \u0026gt;}}$...${{\u0026lt; /math \u0026gt;}} or {{\u0026lt; math \u0026gt;}}$$...$${{\u0026lt; /math \u0026gt;}}, respectively. (We wrap the LaTeX math in the Wowchemy math shortcode to prevent Hugo rendering our math as Markdown. The math shortcode is new in v5.5-dev.)\nExample math block:\n{{\u0026lt; math \u0026gt;}} $$ \\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2} $$ {{\u0026lt; /math \u0026gt;}} renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ Example inline math {{\u0026lt; math \u0026gt;}}$\\nabla F(\\mathbf{x}_{n})${{\u0026lt; /math \u0026gt;}} renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the math linebreak (\\\\):\n{{\u0026lt; math \u0026gt;}} $$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$ {{\u0026lt; /math \u0026gt;}} renders as\n$$ f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases} $$ Diagrams Wowchemy supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ``` renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ``` renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ``` renders as\ngantt section …","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://chao-peng.github.io/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Hugo Blox Builder is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Markdown","type":"post"},{"authors":["Chao Peng"],"categories":[],"content":"","date":1559170800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559170800,"objectID":"5d898596eb898f48d5c2d29ca29b2325","permalink":"https://chao-peng.github.io/publication/issta19ds/","publishdate":"2019-05-30T00:00:00+01:00","relpermalink":"/publication/issta19ds/","section":"publication","summary":"Testing is an important and challenging part of software development and its effectiveness depends on the quality of test cases. However, there exists no means of measuring quality of tests developed for GPU programs and as a result, no test case generation techniques for GPU programs aiming at high test effectiveness. Existing criteria for sequential and threaded CPU programs cannot be directly applied to GPU programs as GPU follows a completely different memory and execution model.\n\nWe surveyed existing work on GPU program verification and bug fixes of open source GPU programs. Based on our findings, we define barrier, branch and loop coverage criteria and propose a set of mutation operators to measure fault finding capabilities of test cases. CLTestCheck, a framework for measuring quality of tests developed for GPU programs by code coverage analysis, fault seeding and work-group schedule amplification has been developed and evaluated using industry standard benchmarks. Experiments show that the framework is able to automatically measure test effectiveness and reveal unusual behaviours. Our planned work includes data flow coverage adopted for GPU programs to probe the underlying cause of unusual kernel behaviours and a more comprehensive work-group scheduler. We also plan to design and develop an automatic test case generator aiming at generating high quality test suites for GPU programs.","tags":["Software Testing","Code Coverage","Fault Finding","Data Race","GPU","OpenCL","Test Case Generation"],"title":"On the Correctness of GPU Programs","type":"publication"},{"authors":["Chao Peng","Ajitha Rajan"],"categories":[],"content":"","date":1554505200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554505200,"objectID":"699cabdba03548ced0dbb442d09aec18","permalink":"https://chao-peng.github.io/publication/fase19/","publishdate":"2019-04-06T00:00:00+01:00","relpermalink":"/publication/fase19/","section":"publication","summary":"Massive parallelism, and energy efficiency of GPUs, along with advances in their programmability with OpenCL and CUDA programming models have made them attractive for general-purpose computations across many application domains. Techniques for testing GPU kernels have emerged recently to aid the construction of correct GPU software. However, there exists no means of measuring quality and effectiveness of tests developed for GPU kernels. Traditional coverage criteria over CPU programs is not adequate over GPU kernels as it uses a completely different programming model and the faults encountered may be specific to the GPU architecture. GPUs have SIMT (single instruction, multiple thread) execution model that executes batches of threads (work groups) in lock-step, i.e all threads in a work group execute the same instruction but on different data.\n\nWe address this need in this paper and present a framework, CLTestCheck, for assessing quality of test suites developed for OpenCL kernels. The framework has the following capabilities, 1. Measures kernel code coverage using three different coverage metrics that are inspired by faults found in real kernel code, 2. Seeds different types of faults in kernel code and measures fault finding capability of test suite, 3. Simulates different work group schedules to check for potential data races with the given test suite. We conducted empirical evaluation of CLTestCheck on a collection of 82 publicly available GPU kernels and test suites. We found that CLTestCheck is capable of automatically measuring effectiveness of test suites, in terms of kernel code coverage, fault finding and revealing data races in real OpenCL kernels.","tags":["Testing","Code Coverage","Fault Finding","Data Race","Mutation Testing","GPU","OpenCL"],"title":"CLTestCheck: Measuring Test Effectiveness for GPU Kernels","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Hugo Blox Builder Hugo Blox Builder | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://chao-peng.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Hugo Blox Builder's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://chao-peng.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://chao-peng.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"}]